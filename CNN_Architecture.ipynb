{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **CNN Architecture | Vikash Kumar | wiryvikash15@gmail.com**"
      ],
      "metadata": {
        "id": "YjWOEC-5Fmsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is the role of filters and feature maps in Convolutional Neural Network (CNN)?**"
      ],
      "metadata": {
        "id": "R_EdvdY0LnjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a Convolutional Neural Network (CNN), filters and feature maps are the core components that enable the network to learn and detect patterns in input data (like images).\n",
        "\n",
        "**Filters (or Kernels):**\n",
        "\n",
        "- A filter is a small matrix of learnable weights. For example, a $3 \\times 3$ or $5 \\times 5$ matrix.\n",
        "\n",
        "- Role: Its role is to act as a feature detector. Each filter is trained to detect a specific, low-level feature, such as a vertical edge, a horizontal edge, a specific color, a corner, or a simple texture.\n",
        "\n",
        "- Process: The filter \"slides\" (or convolves) across the entire input image, one patch at a time. At each position, it computes a dot product between its own weights and the pixel values of the image patch it's currently covering. This operation produces a single number.\n",
        "\n",
        "**Feature Maps (or Activation Maps):**\n",
        "\n",
        "- A feature map is the 2D matrix (an image) that results from applying one filter across the entire input.\n",
        "\n",
        "- Role: Its role is to show the presence and location of the specific feature that the filter was trained to detect.\n",
        "\n",
        "- Process: The collection of numbers from the filter's dot products at every position forms the feature map. A high-value (high activation) in the feature map means the feature was strongly detected at that location in the input. A low value means the feature was not present.\n",
        "\n",
        "- A single convolutional layer typically learns multiple filters (e.g., 32 or 64), each detecting a different feature. Therefore, a convolutional layer produces a stack of 32 or 64 feature maps as its output, one for each filter."
      ],
      "metadata": {
        "id": "qMMjwyoCLxLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Explain the concepts of padding and stride in CNNs (Convolutional Neural Network). How do they affect the output dimensions of feature maps?**"
      ],
      "metadata": {
        "id": "SQb3g9sgL_Cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding and stride are two crucial hyperparameters in a convolutional layer that control the mechanics of the convolution operation and the spatial size of the resulting feature map.\n",
        "\n",
        "**Padding:**\n",
        "\n",
        "   - Concept: Padding refers to the process of adding extra pixels (usually with a value of 0) around the border of an input image or feature map before the convolution operation.\n",
        "\n",
        "   - Purpose:\n",
        "   \n",
        "      - Preserving Spatial Dimensions: Without padding, each convolution operation shrinks the output size. This rapid shrinking limits the number of layers a network can have. Padding (specifically \"same\" padding) allows the output feature map to have the same width and height as the input.\n",
        "      - Improving Edge Detection: Filters can only be centered on pixels that are not on the edge. This means pixels at the very border of the image are processed fewer times than pixels in the center, and their information is partially lost. Padding allows the filter to be centered on border pixels, ensuring they are given full consideration.\n",
        "\n",
        "**Stride:**\n",
        "\n",
        "  - Concept: Stride defines the number of pixels the filter \"steps\" or \"slides\" over the input at a time. A stride of 1 ($S=1$) means the filter moves one pixel at a time (horizontally or vertically). A stride of 2 ($S=2$) means it skips every other pixel, moving two pixels at a time.\n",
        "  - Purpose: Stride is primarily used for downsampling. A stride greater than 1 significantly reduces the spatial dimensions (width and height) of the output feature map, which helps to reduce the computational load and aggregate information over a wider area.\n",
        "\n",
        " **Effect on Output Dimensions:**\n",
        "\n",
        " The spatial dimensions (Width and Height) of the output feature map are calculated by the following formula:$$Output\\_Dimension = \\frac{(W - K + 2P)}{S} + 1$$\n",
        "\n",
        " Where:$W$ = Input dimension (e.g., input width)\n",
        "\n",
        " $K$ = Filter/Kernel dimension (e.g., filter width)\n",
        "\n",
        " $P$ = Padding (number of pixels added to one side)\n",
        "\n",
        " $S$ = StrideAs we can see, increasing padding\n",
        "\n",
        " ($P$) increases the output size, while increasing stride ($S$) decreases the output size."
      ],
      "metadata": {
        "id": "pcAblP5pMGMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Define receptive field in the context of CNNs.Why is it important for deep architectures?**"
      ],
      "metadata": {
        "id": "PfawT2UxMgI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Receptive Field:** The receptive field of a neuron in a CNN (i.e., a single \"pixel\" in a feature map) is the specific region or area in the original input image that this neuron \"sees\" or is affected by.\n",
        "\n",
        "- In the first convolutional layer, a neuron's receptive field is simply the size of the filter (e.g., $3 \\times 3$).\n",
        "\n",
        "- In the second layer, a neuron is looking at a $3 \\times 3$ patch of the first layer's feature map. But each of those 9 neurons in the first layer was looking at a $3 \\times 3$ patch of the original image. Therefore, the neuron in the second layer has an effective receptive field of $5 \\times 5$ on the original input.\n",
        "\n",
        "**Importance in Deep Architectures:** The concept of a growing receptive field is fundamental to why deep (multi-layered) architectures work so well:\n",
        "\n",
        "**1. Hierarchical Feature Learning:** Deep architectures create a hierarchy of features.\n",
        "\n",
        "- Early Layers: Have small receptive fields. They learn to detect simple, local features like edges, corners, and colors.\n",
        "\n",
        "- Middle Layers: Have medium receptive fields. They combine the simple features from earlier layers to learn more complex textures and patterns (e.g., an \"eye\" or a \"wheel\").\n",
        "\n",
        "- Deep Layers: Have large receptive fields (sometimes covering the entire image). They combine the complex patterns from middle layers to detect abstract, high-level objects (e.g., a \"human face\" or a \"car\").\n",
        "\n",
        "\n",
        "**2. Efficiency:** This hierarchical stacking is computationally more efficient than using one single, enormous filter to detect a complex object. It allows the network to learn and share low-level feature detectors (like edge detectors) across many different high-level object concepts.\n",
        "\n",
        "In short, depth in a CNN is a mechanism for systematically increasing the receptive field, allowing the network to learn complex, large-scale patterns from simple, local ones."
      ],
      "metadata": {
        "id": "dVhU96aAMluZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Discuss how filter size and stride influence the number of parameters in a CNN.**"
      ],
      "metadata": {
        "id": "JGQcwWczMoXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Parameters are the learnable weights and biases within the network.\n",
        "\n",
        " - Filter Size:Filter size has a direct and significant influence on the number of parameters. The parameters are the weights inside the filter.\n",
        "\n",
        " A $3 \\times 3$ filter has $3 \\times 3 = 9$ parameters (weights).\n",
        "\n",
        " A $5 \\times 5$ filter has $5 \\times 5 = 25$ parameters.\n",
        "\n",
        " A $7 \\times 7$ filter has $7 \\times 7 = 49$ parameters.\n",
        "\n",
        " The number of parameters also scales with the depth of the input (number of input channels, $C_{in}$). A $3 \\times 3$ filter on an RGB image (3 channels) actually has $3 \\times 3 \\times 3 = 27$ weights.\n",
        "\n",
        " The total parameters for a single filter (plus its one bias) is: (filter_height * filter_width * C_in) + 1_bias.\n",
        "\n",
        " If the layer has $N$ filters (to produce $N$ feature maps), the total parameters for the layer is: $N \\times ((filter\\_height \\times filter\\_width \\times C_{in}) + 1)$.\n",
        "\n",
        " Conclusion: Larger filter sizes dramatically increase the number of parameters.\n",
        "\n",
        " **Stride:** Stride has no influence at all on the number of parameters.\n",
        "\n",
        " Explanation: The parameters are the weights inside the filter, and these weights are fixed (for that pass) regardless of how the filter moves. Stride only dictates where the filter is applied and how many \"steps\" it takes to cross the image.\n",
        "\n",
        " Effect: Stride only affects the spatial dimensions (width and height) of the output feature map. A larger stride creates a smaller output, but the number of weights used to create that output remains the same.In summary: Filter size (along with input channels and number of filters) determines the parameter count. Stride determines the output size."
      ],
      "metadata": {
        "id": "1Y-Yy_C5Mt89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Compare and contrast different CNN-based architectures like LeNet, AlexNet, and VGG in terms of depth, filter sizes, and performance.**"
      ],
      "metadata": {
        "id": "9aun1qobMwa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet, AlexNet, and VGG represent key evolutionary steps in the development of deep CNNs.\n",
        "\n",
        "**LeNet (LeNet-5)**\n",
        "- Year: 1998\n",
        "- Depth: Shallow (5 layers: 2 Conv, 3 FC)\n",
        "- Filter Sizes: Primarily $5 \\times 5$ filters.\n",
        "- Activation: Sigmoid or Tanh\n",
        "- Key Innovations: The first widely successful CNN. Proved the concept of stacked Conv + Pooling layers for feature extraction. Used for digit recognition.\n",
        "- Performance: State-of-the-art for its time on simple tasks (like MNIST digits).\n",
        "- Parameters: ~60,000\n",
        "\n",
        "\n",
        "\n",
        "**AlexNet**\n",
        "- 2012\n",
        "- Deeper (8 layers: 5 Conv, 3 FC)\n",
        "- Varied sizes. Large $11 \\times 11$ in the first layer, then $5 \\times 5$ and $3 \\times 3$.\n",
        "- ReLU (Rectified Linear Unit). This was a key innovation that solved the vanishing gradient problem and allowed for deeper models.\n",
        "- Won the 2012 ImageNet competition, proving CNNs' dominance. Used ReLU, Dropout (for regularization), and data augmentation.\n",
        "- Groundbreaking performance on a complex dataset (ImageNet). Top-5 error of 15.3%.\n",
        "- ~60 Million\n",
        "\n",
        "**VGG (VGG-16/19)**\n",
        "- 2014\n",
        "- Very Deep (16 or 19 layers: 13/16 Conv, 3 FC)\n",
        "- Standardized on very small $3 \\times 3$ filters stacked together.\n",
        "- ReLU.\n",
        "- Proved that depth is critical. Its simple, homogeneous architecture (just stacking $3 \\times 3$ conv blocks) showed that networks could be built much deeper and achieve better performance.\n",
        "- State-of-the-art in 2014. Improved on AlexNet (Top-5 error of ~7.3%). VGG's simplicity and performance made it a very popular \"backbone\" for many other tasks (like object detection and segmentation).\n",
        "- ~138 Million (VGG-16). Very \"heavy\" model with many parameters, mostly in the fully connected layers."
      ],
      "metadata": {
        "id": "dG4MNOhaM4r0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contrast Summary:**\n",
        "\n",
        "- LeNet was the \"proof of concept\" on a small scale.\n",
        "\n",
        "- AlexNet was the \"breakthrough,\" proving CNNs could scale to complex problems by introducing key components like ReLU and Dropout.\n",
        "\n",
        "- VGG was the \"scaling\" architecture, demonstrating that a simple, standardized, and very deep stack of small filters was a highly effective and generalizable design."
      ],
      "metadata": {
        "id": "vScFghEXM89X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Using keras, build and train a simple CNN model on the MNIST dataset from scratch.Include code for module creation, compilation, training, and evaluation.**"
      ],
      "metadata": {
        "id": "cXckU0teNAs3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NAvBVLxXLiUJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "outputId": "dc4ea746-d35a-4773-e53a-4abf84042c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training data shape: (60000, 28, 28, 1)\n",
            "Test data shape: (10000, 28, 28, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Model Training ---\n",
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 64ms/step - accuracy: 0.8949 - loss: 0.3674 - val_accuracy: 0.9815 - val_loss: 0.0549\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 63ms/step - accuracy: 0.9856 - loss: 0.0461 - val_accuracy: 0.9836 - val_loss: 0.0508\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 60ms/step - accuracy: 0.9899 - loss: 0.0315 - val_accuracy: 0.9880 - val_loss: 0.0363\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 61ms/step - accuracy: 0.9924 - loss: 0.0231 - val_accuracy: 0.9906 - val_loss: 0.0302\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 64ms/step - accuracy: 0.9951 - loss: 0.0155 - val_accuracy: 0.9913 - val_loss: 0.0283\n",
            "--- Model Training Finished ---\n",
            "\n",
            "--- Evaluating Model on Test Data ---\n",
            "313/313 - 3s - 10ms/step - accuracy: 0.9913 - loss: 0.0283\n",
            "\n",
            "Test Loss: 0.0283\n",
            "Test Accuracy: 99.13%\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# Load the MNIST dataset (handwritten digits)\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the images\n",
        "# Reshape to include the channel dimension (1 for grayscale)\n",
        "#    MNIST images are 28x28 pixels\n",
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
        "\n",
        "#  Normalize pixel values from 0-255 to 0.0-1.0\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Preprocess the labels (target)\n",
        "# We use 'sparse_categorical_crossentropy' as the loss function,\n",
        "# which accepts integer labels directly, so one-hot encoding is not strictly needed.\n",
        "# If we used 'categorical_crossentropy', we would uncomment the lines below:\n",
        "# y_train = to_categorical(y_train, 10)\n",
        "# y_test = to_categorical(y_test, 10)\n",
        "\n",
        "print(f\"Training data shape: {x_train.shape}\")\n",
        "print(f\"Test data shape: {x_test.shape}\")\n",
        "\n",
        "# --- Build the CNN Model (Module Creation) ---\n",
        "\n",
        "model = Sequential([\n",
        "    # Define the input shape in the first layer\n",
        "    Input(shape=(28, 28, 1)),\n",
        "\n",
        "    # Convolutional Layer 1\n",
        "    # 32 filters, 3x3 kernel size, ReLU activation\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
        "\n",
        "    # Pooling Layer 1\n",
        "    # 2x2 pooling window\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Convolutional Layer 2\n",
        "    # 64 filters, 3x3 kernel\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "\n",
        "    # Pooling Layer 2\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Flatten the 3D feature maps into a 1D vector\n",
        "    Flatten(),\n",
        "\n",
        "    # Fully Connected (Dense) Layer\n",
        "    Dense(128, activation='relu'),\n",
        "\n",
        "    # Output Layer\n",
        "    # 10 units (one for each digit 0-9), softmax for multi-class probability\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Display the model architecture\n",
        "model.summary()\n",
        "\n",
        "# ---  Compile the Model ---\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',                         # Efficient optimizer\n",
        "    loss='sparse_categorical_crossentropy',   # Loss function for integer labels\n",
        "    metrics=['accuracy']                      # Metric to monitor\n",
        ")\n",
        "\n",
        "# ---  Train the Model ---\n",
        "\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=5,                                # Number of times to iterate over the dataset\n",
        "    batch_size=64,                           # Number of samples per gradient update\n",
        "    validation_data=(x_test, y_test)         # Data to evaluate against at the end of each epoch\n",
        ")\n",
        "print(\"--- Model Training Finished ---\")\n",
        "\n",
        "# ---  Evaluate the Model ---\n",
        "\n",
        "print(\"\\n--- Evaluating Model on Test Data ---\")\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images.Show your preprocessing and architecture.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mC54hAKYxTXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load the CIFAR-10 dataset (32x32 color images in 10 classes)\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# CIFAR-10 images are 32x32x3 (RGB), so no reshaping is needed\n",
        "# Class names for reference\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Preprocessing: Normalize pixel values from 0-255 to 0.0-1.0\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Labels are already integers (0-9), which 'sparse_categorical_crossentropy' can use.\n",
        "\n",
        "print(f\"Training data shape: {x_train.shape}\")\n",
        "print(f\"Test data shape: {x_test.shape}\")\n",
        "\n",
        "# ---  Create the CNN Model Architecture ---\n",
        "# This model needs to be deeper than the MNIST one, as CIFAR-10 is a more complex dataset.\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(32, 32, 3)), # 32x32 pixels with 3 color channels\n",
        "\n",
        "    # Block 1\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25), # Add dropout for regularization\n",
        "\n",
        "    # Block 2\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # Flatten and Dense Layers\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax') # 10 output classes\n",
        "])\n",
        "\n",
        "# Display the model architecture\n",
        "model.summary()\n",
        "\n",
        "# ---  Compile the Model ---\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ---  Train the Model ---\n",
        "\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=15,  # Needs more epochs than MNIST\n",
        "    batch_size=64,\n",
        "    validation_data=(x_test, y_test)\n",
        ")\n",
        "print(\"--- Model Training Finished ---\")\n",
        "\n",
        "# ---  Evaluate the Model ---\n",
        "\n",
        "print(\"\\n--- Evaluating Model on Test Data ---\")\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c6s99_xSxPhk",
        "outputId": "6c453393-dd9d-4600-8e45-3172decd78f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Training data shape: (50000, 32, 32, 3)\n",
            "Test data shape: (10000, 32, 32, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m2,097,664\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,130\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,664</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,168,362\u001b[0m (8.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,168,362</span> (8.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,168,362\u001b[0m (8.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,168,362</span> (8.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Model Training ---\n",
            "Epoch 1/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - accuracy: 0.3392 - loss: 1.7884 - val_accuracy: 0.5903 - val_loss: 1.1578\n",
            "Epoch 2/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.5836 - loss: 1.1727 - val_accuracy: 0.6637 - val_loss: 0.9476\n",
            "Epoch 3/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.6644 - loss: 0.9504 - val_accuracy: 0.6992 - val_loss: 0.8655\n",
            "Epoch 4/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7057 - loss: 0.8386 - val_accuracy: 0.7279 - val_loss: 0.7770\n",
            "Epoch 5/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7373 - loss: 0.7471 - val_accuracy: 0.7430 - val_loss: 0.7386\n",
            "Epoch 6/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7572 - loss: 0.6911 - val_accuracy: 0.7373 - val_loss: 0.7479\n",
            "Epoch 7/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7693 - loss: 0.6446 - val_accuracy: 0.7641 - val_loss: 0.6911\n",
            "Epoch 8/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7887 - loss: 0.5987 - val_accuracy: 0.7703 - val_loss: 0.6670\n",
            "Epoch 9/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7999 - loss: 0.5616 - val_accuracy: 0.7719 - val_loss: 0.6520\n",
            "Epoch 10/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.8131 - loss: 0.5317 - val_accuracy: 0.7832 - val_loss: 0.6453\n",
            "Epoch 11/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8221 - loss: 0.5023 - val_accuracy: 0.7730 - val_loss: 0.6737\n",
            "Epoch 12/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8297 - loss: 0.4799 - val_accuracy: 0.7845 - val_loss: 0.6459\n",
            "Epoch 13/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8334 - loss: 0.4630 - val_accuracy: 0.7865 - val_loss: 0.6272\n",
            "Epoch 14/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8468 - loss: 0.4358 - val_accuracy: 0.7864 - val_loss: 0.6486\n",
            "Epoch 15/15\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8484 - loss: 0.4287 - val_accuracy: 0.7881 - val_loss: 0.6657\n",
            "--- Model Training Finished ---\n",
            "\n",
            "--- Evaluating Model on Test Data ---\n",
            "313/313 - 2s - 5ms/step - accuracy: 0.7881 - loss: 0.6657\n",
            "\n",
            "Test Loss: 0.6657\n",
            "Test Accuracy: 78.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Using PyTorch, write a script to define and train a CNN on the MNIST dataset.\n",
        "Include model definition, data loaders, training loop, and accuracy evaluation**"
      ],
      "metadata": {
        "id": "aO5XHMY-xfgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- Setup Device (GPU or CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Input channel = 1 (grayscale), Output channels = 32\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        # Input = 32, Output = 64\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Max pooling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # Halves the size\n",
        "\n",
        "        # After conv1 + pool -> 28x28 -> 14x14\n",
        "        # After conv2 + pool -> 14x14 -> 7x7\n",
        "        # 64 channels * 7x7 image size\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10) # 10 output classes\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv 1 -> ReLU -> Pool\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        # Conv 2 -> ReLU -> Pool\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        # Flatten the tensor for the dense layer\n",
        "        x = x.view(-1, 64 * 7 * 7) # -1 infers the batch size\n",
        "\n",
        "        # Dense 1 -> ReLU -> Dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        # Dense 2 (Output)\n",
        "        x = self.fc2(x)\n",
        "        # CrossEntropyLoss applies log_softmax internally\n",
        "        return x\n",
        "\n",
        "# Instantiate the model and move it to the device\n",
        "model = Net().to(device)\n",
        "print(model)\n",
        "\n",
        "# ---  Prepare Data Loaders ---\n",
        "\n",
        "# Transformations: Convert to Tensor and Normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # Mean/Stddev for MNIST\n",
        "])\n",
        "\n",
        "# Download and load training data\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load test data\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# ---  Define Loss Function and Optimizer ---\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ---  Training Loop ---\n",
        "\n",
        "def train(epoch):\n",
        "    model.train() # Set the model to training mode\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Move data to the device (GPU/CPU)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # 1. Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # 3. Calculate loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # 4. Backward pass (compute gradients)\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n",
        "                  f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "# --- Accuracy Evaluation Loop ---\n",
        "\n",
        "def test():\n",
        "    model.eval() # Set the model to evaluation mode (disables dropout, etc.)\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item() # Sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # Get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "          f' ({accuracy:.2f}%)\\n')\n",
        "\n",
        "# --- Run Training and Evaluation ---\n",
        "num_epochs = 5\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bqa3aDYxax0",
        "outputId": "e70efda6-1efa-4c87-f5cb-720e0cc58a04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 484kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.47MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.77MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.310206\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.219465\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.147292\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.124825\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.048663\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.123484\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.111625\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.107420\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.084756\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.159622\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9871/10000 (98.71%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.098658\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.028619\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.009094\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.072032\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.066476\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.242973\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.087835\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.013189\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.006856\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.010577\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9900/10000 (99.00%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.025412\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.050801\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.157968\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.043140\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.031906\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.020853\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.164197\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.021968\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.023091\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.004989\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9897/10000 (98.97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.062765\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.009893\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.013072\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.001983\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.003568\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.030119\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.001188\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.032375\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.028318\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.007210\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9896/10000 (98.96%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.051846\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.062123\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.007825\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.005078\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.008400\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.013247\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.051333\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.138823\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.005766\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.001850\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9913/10000 (99.13%)\n",
            "\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.**"
      ],
      "metadata": {
        "id": "U03KgIkIxul0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create the directory structure\n",
        "os.makedirs('./dataset/train/class_A', exist_ok=True)\n",
        "os.makedirs('./dataset/train/class_B', exist_ok=True)\n",
        "os.makedirs('./dataset/validation/class_A', exist_ok=True)\n",
        "os.makedirs('./dataset/validation/class_B', exist_ok=True)\n",
        "\n",
        "print(\"Dummy folders created!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFk-6A0S6ebk",
        "outputId": "0468edf0-01e0-42a8-d392-232625ec1bbc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy folders created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "\n",
        "train_dir = './dataset/train'\n",
        "validation_dir = './dataset/validation'\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# ---  Create ImageDataGenerators ---\n",
        "# We apply data augmentation to the training data to prevent overfitting.\n",
        "# We ONLY rescale the validation data (no augmentation).\n",
        "\n",
        "print(\"Setting up data generators...\")\n",
        "# Training Data Generator with Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,            # Normalize pixel values to [0, 1]\n",
        "    rotation_range=40,         # Randomly rotate images\n",
        "    width_shift_range=0.2,     # Randomly shift width\n",
        "    height_shift_range=0.2,    # Randomly shift height\n",
        "    shear_range=0.2,           # Apply shear transformation\n",
        "    zoom_range=0.2,            # Randomly zoom in\n",
        "    horizontal_flip=True,      # Randomly flip horizontally\n",
        "    fill_mode='nearest'        # Strategy for filling new pixels\n",
        ")\n",
        "\n",
        "# Validation Data Generator (only rescaling)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# --- Create Data Flows from Directories ---\n",
        "\n",
        "# 'flow_from_directory' links the generator to the directory structure\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH), # Resize all images to 150x150\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'  # 'binary' for 2 classes, 'categorical' for >2\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# --- Build a CNN Model ---\n",
        "# (Using a simple model for demonstration)\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)), # 150x150 RGB images\n",
        "\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid') # 1 neuron + sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# ---  Compile the Model ---\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy', # Use binary_crossentropy for 2 classes\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# --- Train the Model using the Generators ---\n",
        "# We use 'fit' (which accepts generators) instead of 'fit_generator'\n",
        "# We must specify steps_per_epoch and validation_steps\n",
        "\n",
        "# These are often calculated as:\n",
        "# steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
        "# validation_steps = validation_generator.samples // BATCH_SIZE\n",
        "# For this example, we'll hardcode them.\n",
        "\n",
        "print(\"\\n--- Starting Model Training with Generators ---\")\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=20,  # Adjust based on the dataset size (e.g., total_train_images // BATCH_SIZE)\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=10  # Adjust based on the dataset size (e.g., total_val_images // BATCH_SIZE)\n",
        ")\n",
        "print(\"--- Model Training Finished ---\")\n",
        "\n",
        "# --- Evaluate the Model ---\n",
        "print(\"\\n--- Evaluating Model ---\")\n",
        "loss, accuracy = model.evaluate(validation_generator, steps=10)\n",
        "print(f\"Validation Loss: {loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U_asJXM4xq0E",
        "outputId": "d604d32e-58f6-4b25-b43f-b05610947479"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up data generators...\n",
            "Found 39 images belonging to 2 classes.\n",
            "Found 10 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m18,940,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m513\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">18,940,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,034,177\u001b[0m (72.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,034,177</span> (72.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,034,177\u001b[0m (72.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,034,177</span> (72.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Model Training with Generators ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m 2/20\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 5s/step - accuracy: 0.6850 - loss: 4.7487"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 478ms/step - accuracy: 0.5300 - loss: 8.4677 - val_accuracy: 0.5000 - val_loss: 1.7194\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 358ms/step - accuracy: 0.5158 - loss: 1.6651 - val_accuracy: 0.5000 - val_loss: 0.9716\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 150ms/step - accuracy: 0.4344 - loss: 0.9526 - val_accuracy: 0.5000 - val_loss: 0.7029\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 152ms/step - accuracy: 0.5365 - loss: 0.6927 - val_accuracy: 0.6000 - val_loss: 0.7032\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 339ms/step - accuracy: 0.5401 - loss: 0.6845 - val_accuracy: 0.5000 - val_loss: 0.7075\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 393ms/step - accuracy: 0.5573 - loss: 0.6745 - val_accuracy: 0.5000 - val_loss: 0.7202\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 210ms/step - accuracy: 0.7439 - loss: 0.6526 - val_accuracy: 0.5000 - val_loss: 0.7255\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 125ms/step - accuracy: 0.6418 - loss: 0.6400 - val_accuracy: 0.7000 - val_loss: 0.7222\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 121ms/step - accuracy: 0.6708 - loss: 0.6455 - val_accuracy: 0.4000 - val_loss: 0.7856\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 362ms/step - accuracy: 0.5716 - loss: 0.7716 - val_accuracy: 0.5000 - val_loss: 0.9453\n",
            "--- Model Training Finished ---\n",
            "\n",
            "--- Evaluating Model ---\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.5000 - loss: 0.9453\n",
            "Validation Loss: 0.9453\n",
            "Validation Accuracy: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. You are working on a web application for a medical imaging startup. Your task is to build and deploy a CNN model that classifies chest X-ray images into \"Normal\" and \"Pneumonia\" categories. Describe your end-to-end approach-from data preparation and model training to deploying the model as a web app using Streamlit.**"
      ],
      "metadata": {
        "id": "dtTZtAMVx5_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a complete, end-to-end project. The approach is broken into two main parts: Part 1 (Model Training) and Part 2 (Streamlit Deployment).\n",
        "\n",
        "**Part 1: End-to-End Approach (Model Training)**\n",
        "\n",
        "The approach would prioritize Transfer Learning, as medical imaging datasets are often relatively small, and pre-trained models capture low-level features (edges, textures) that are highly relevant to X-rays.\n",
        "\n",
        "1. Data Preparation:\n",
        "\n",
        "  - Sourcing: We would use a publicly available dataset, such as the \"Chest X-Ray Images (Pneumonia)\" dataset from Kaggle.\n",
        "\n",
        "  - Organization:Structure the data into the Keras-compatible directory format:"
      ],
      "metadata": {
        "id": "yfl3BfWryC_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "chest_xray/\n",
        "\n",
        "    train/\n",
        "        NORMAL/\n",
        "        PNEUMONIA/\n",
        "    test/\n",
        "        NORMAL/\n",
        "        PNEUMONIA/\n",
        "    val/\n",
        "        NORMAL/\n",
        "        PNEUMONIA/"
      ],
      "metadata": {
        "id": "2pskiiF2yFqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing (using ImageDataGenerator):**\n",
        "\n",
        "  - Training Generator: Apply resizing (e.g., to $224 \\times 224$ to match VGG/ResNet), rescaling (1./255), and data augmentation (slight rotations, width/height shifts, zoom). This is crucial for handling class imbalance (if any) and making the model robust.\n",
        "  \n",
        "  - Validation/Test Generator: Apply only resizing and rescaling.\n",
        "  \n",
        " **2. Model Building (Transfer Learning):**\n",
        "  \n",
        "  - Base Model: We'd select a powerful, pre-trained model like VGG16, ResNet50, or MobileNetV2 (good for fast inference). Let's use MobileNetV2.\n",
        "  \n",
        "  - Loading: We will load the MobileNetV2 base, pre-trained on imagenet, setting include_top=False (to remove its original classification layer) and specifying the input_shape=(224, 224, 3).\n",
        "  \n",
        "  - Freezing: We will \"freeze\" the weights of the base model (base_model.trainable = False) so that only my new, custom classification head is trained during the initial phase.\n",
        "  \n",
        "  - Building the \"Head\": we will stack new layers on top of the base model's output:\n",
        "  \n",
        "    - GlobalAveragePooling2D(): To flatten the feature maps.\n",
        "    \n",
        "    - Dense(128, activation='relu'): A hidden dense layer.\n",
        "    \n",
        "    - Dropout(0.5): For regularization.\n",
        "    \n",
        "    - Dense(1, activation='sigmoid'): The final output layer (1 neuron + sigmoid for \"Normal\" vs. \"Pneumonia\" binary classification).\n",
        "    \n",
        "  **Model Training & Saving:**\n",
        "  \n",
        "  - Compilation: We will compile the model using optimizer='adam', loss='binary_crossentropy', and metrics=['accuracy'].\n",
        "  \n",
        "  - Training: We will train the model using model.fit() with the train_generator and validation_generator. we would also use callbacks like EarlyStopping (to prevent overfitting) and ModelCheckpoint (to save the best model).\n",
        "  \n",
        "  - Fine-Tuning (Optional): After the head is trained,We might \"unfreeze\" the top few layers of the base model and re-train at a very low learning rate to fine-tune it specifically for X-ray features.\n",
        "  \n",
        "  - Saving: Once training is complete,we will save the final, best-performing model: model.save('pneumonia_classifier_model.h5')."
      ],
      "metadata": {
        "id": "Ax0oywUWyJhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2: Python Code (Training & Deployment)**\n",
        "\n",
        "This requires two separate Python files.\n",
        "\n",
        "**File 1: train_model.py (The Model Training Script)**"
      ],
      "metadata": {
        "id": "hmcaZDUCyM2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Define paths\n",
        "base_dir = 'chest_xray'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(os.path.join(train_dir, 'NORMAL'), exist_ok=True)\n",
        "os.makedirs(os.path.join(train_dir, 'PNEUMONIA'), exist_ok=True)\n",
        "os.makedirs(os.path.join(val_dir, 'NORMAL'), exist_ok=True)\n",
        "os.makedirs(os.path.join(val_dir, 'PNEUMONIA'), exist_ok=True)\n",
        "os.makedirs(os.path.join(test_dir, 'NORMAL'), exist_ok=True)\n",
        "os.makedirs(os.path.join(test_dir, 'PNEUMONIA'), exist_ok=True)\n",
        "\n",
        "# Function to create dummy images (224x224, as expected by MobileNetV2)\n",
        "def create_dummy_xray_image(path, color):\n",
        "    img = Image.new('RGB', (224, 224), color=color)\n",
        "    img.save(path)\n",
        "\n",
        "# Create a few dummy images\n",
        "create_dummy_xray_image(os.path.join(train_dir, 'NORMAL', 'dummy_norm_1.jpeg'), 'gray')\n",
        "create_dummy_xray_image(os.path.join(train_dir, 'NORMAL', 'dummy_norm_2.jpeg'), 'gray')\n",
        "create_dummy_xray_image(os.path.join(train_dir, 'PNEUMONIA', 'dummy_pneu_1.jpeg'), 'black')\n",
        "create_dummy_xray_image(os.path.join(train_dir, 'PNEUMONIA', 'dummy_pneu_2.jpeg'), 'black')\n",
        "\n",
        "create_dummy_xray_image(os.path.join(val_dir, 'NORMAL', 'dummy_norm_val_1.jpeg'), 'gray')\n",
        "create_dummy_xray_image(os.path.join(val_dir, 'PNEUMONIA', 'dummy_pneu_val_1.jpeg'), 'black')\n",
        "\n",
        "create_dummy_xray_image(os.path.join(test_dir, 'NORMAL', 'dummy_norm_test_1.jpeg'), 'gray')\n",
        "create_dummy_xray_image(os.path.join(test_dir, 'PNEUMONIA', 'dummy_pneu_test_1.jpeg'), 'black')\n",
        "\n",
        "\n",
        "print(\"Folder structure:\")\n",
        "print(\"./chest_xray\")\n",
        "print(\"  .../train/NORMAL\")\n",
        "print(\"  .../train/PNEUMONIA\")\n",
        "print(\"  .../val/NORMAL\")\n",
        "print(\"  .../val/PNEUMONIA\")\n",
        "print(\"  .../test/NORMAL\")\n",
        "print(\"  .../test/PNEUMONIA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXoaNUkjAf5R",
        "outputId": "851ee0c2-81ed-4c5d-f9c7-dd579268a342"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder structure:\n",
            "./chest_xray\n",
            "  .../train/NORMAL\n",
            "  .../train/PNEUMONIA\n",
            "  .../val/NORMAL\n",
            "  .../val/PNEUMONIA\n",
            "  .../test/NORMAL\n",
            "  .../test/PNEUMONIA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vikash Kumar\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYfIZe2mIsex",
        "outputId": "c9bc4865-79cf-4fd0-a6bb-67a230675f2d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vikash Kumar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "TRAIN_DIR = 'chest_xray/train'\n",
        "VAL_DIR = 'chest_xray/val'\n",
        "TEST_DIR = 'chest_xray/test'\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    VAL_DIR,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# --- Model Building (Transfer Learning) ---\n",
        "\n",
        "# Load base model\n",
        "base_model = MobileNetV2(\n",
        "    input_shape=(*IMG_SIZE, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_model.trainable = False # Freeze the base\n",
        "\n",
        "# Add custom head\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x) # Binary output\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# --- Compile and Train ---\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"--- Starting Training ---\")\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# --- Save the Model ---\n",
        "model.save('pneumonia_classifier_model.h5')\n",
        "print(\"Model saved as 'pneumonia_classifier_model.h5'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbx9aktAx0hv",
        "outputId": "5c7a96aa-cf9a-41c7-b0b8-ed3346a5f508"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3959 images belonging to 2 classes.\n",
            "Found 2 images belonging to 2 classes.\n",
            "--- Starting Training ---\n",
            "Epoch 1/10\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 1s/step - accuracy: 0.7296 - loss: 0.5434 - val_accuracy: 0.5000 - val_loss: 1.1289\n",
            "Epoch 2/10\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 825ms/step - accuracy: 0.9125 - loss: 0.2208 - val_accuracy: 0.5000 - val_loss: 1.2179\n",
            "Epoch 3/10\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 838ms/step - accuracy: 0.9149 - loss: 0.2115 - val_accuracy: 0.5000 - val_loss: 0.8969\n",
            "Epoch 4/10\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 814ms/step - accuracy: 0.9289 - loss: 0.1756 - val_accuracy: 0.5000 - val_loss: 0.8393\n",
            "Epoch 5/10\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 773ms/step - accuracy: 0.9338 - loss: 0.1641 - val_accuracy: 0.5000 - val_loss: 0.7347\n",
            "Epoch 6/10\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 599ms/step - accuracy: 0.9455 - loss: 0.1459 - val_accuracy: 0.5000 - val_loss: 0.7750\n",
            "Epoch 7/10\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 595ms/step - accuracy: 0.9414 - loss: 0.1498 - val_accuracy: 0.5000 - val_loss: 0.4974\n",
            "Epoch 8/10\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 595ms/step - accuracy: 0.9442 - loss: 0.1396 - val_accuracy: 0.5000 - val_loss: 0.4899\n",
            "Epoch 9/10\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 596ms/step - accuracy: 0.9418 - loss: 0.1400 - val_accuracy: 1.0000 - val_loss: 0.3918\n",
            "Epoch 10/10\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 602ms/step - accuracy: 0.9517 - loss: 0.1274 - val_accuracy: 1.0000 - val_loss: 0.3541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as 'pneumonia_classifier_model.h5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**File 2: app.py (The Streamlit Web App)**\n",
        "\n",
        "(To run this: pip install streamlit and then streamlit run app.py)"
      ],
      "metadata": {
        "id": "y7-NiEx3ySUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2VhKuqwPRJb",
        "outputId": "c8460dbd-bfdf-49e5-eabc-2db0683c1830"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.51.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.10.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.28.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.51.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m132.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.51.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# --- Load the Trained Model ---\n",
        "# Use st.cache_resource to load the model only once\n",
        "@st.cache_resource\n",
        "def load_pneumonia_model():\n",
        "    model = load_model('pneumonia_classifier_model.h5')\n",
        "    return model\n",
        "\n",
        "model = load_pneumonia_model()\n",
        "\n",
        "# ---  Helper Function for Preprocessing ---\n",
        "def preprocess_image(image):\n",
        "    # Convert to RGB (in case of RGBA or Grayscale)\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    # Resize to the model's expected input size\n",
        "    image = image.resize((224, 224))\n",
        "\n",
        "    # Convert to numpy array and rescale\n",
        "    image_array = np.asarray(image)\n",
        "    image_array = image_array.astype('float32') / 255.0\n",
        "\n",
        "    # Expand dimensions to create a \"batch\" of 1\n",
        "    image_array = np.expand_dims(image_array, axis=0)\n",
        "    return image_array\n",
        "\n",
        "# ---  Streamlit App UI ---\n",
        "st.title(\"Chest X-Ray Classifier 🩺\")\n",
        "st.write(\"Upload a chest X-ray image to classify it as **Normal** or **Pneumonia**.\")\n",
        "\n",
        "# File uploader widget\n",
        "uploaded_file = st.file_uploader(\"Choose an X-ray image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    #  Display the uploaded image\n",
        "    image = Image.open(uploaded_file)\n",
        "    st.image(image, caption='Uploaded X-Ray', use_column_width=True)\n",
        "\n",
        "    #  Preprocess the image and make prediction\n",
        "    st.write(\"Classifying...\")\n",
        "    processed_image = preprocess_image(image)\n",
        "\n",
        "    #  Make prediction\n",
        "    prediction = model.predict(processed_image)\n",
        "    score = prediction[0][0] # Get the single prediction value\n",
        "\n",
        "    #  Display the result\n",
        "    if score > 0.5:\n",
        "        st.error(f\"**Result: Pneumonia** (Confidence: {score*100:.2f}%)\")\n",
        "    else:\n",
        "        st.success(f\"**Result: Normal** (Confidence: {(1-score)*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k_fXnlvyO2q",
        "outputId": "807b92cd-db38-4b87-d368-6f0dbde4438e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-09 06:10:41.473 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:41.664 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-11-09 06:10:41.670 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:41.672 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:41.675 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:42.187 Thread 'Thread-45': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:42.199 Thread 'Thread-45': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:42.201 Thread 'Thread-45': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "2025-11-09 06:10:43.286 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:43.287 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:43.289 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:43.296 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:43.298 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:43.302 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:43.304 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:43.305 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:43.311 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:43.312 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:43.316 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:43.317 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:43.321 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:43.324 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-11-09 06:10:43.327 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "65JRolXLyT-K",
        "outputId": "3420afe3-7284-4a7e-f603-1960f4f623b8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1609277159.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1609277159.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q2vJIYfHPk_B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}