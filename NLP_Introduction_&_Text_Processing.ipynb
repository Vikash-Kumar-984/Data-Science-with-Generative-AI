{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **NLP Introduction & Text Processing | Vikash Kumar | wiryvikash15@gmail.com**"
      ],
      "metadata": {
        "id": "JRmfpSUkNXQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is Computational Linguistics and how does it relate to NLP?**\n",
        "\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "**Computational Linguistics** is the scientific and engineering discipline concerned with the computational properties of natural language. It involves using formal models and mathematical methods to understand, model, and analyze language phenomena such as syntax, semantics, phonology, and discourse.\n",
        "\n",
        "### Relationship between Computational Linguistics and NLP:\n",
        "\n",
        "**Computational Linguistics** focuses on the **theoretical foundations** and **formal modeling** of language phenomena. It is a scientific field that studies how languages can be represented and processed using computational methods. Key aspects include:\n",
        "\n",
        "1. **Formal Grammar Development**: Creating formal representations of language structure\n",
        "2. **Language Modeling**: Developing probabilistic and statistical models of language\n",
        "3. **Linguistic Research**: Understanding fundamental linguistic phenomena computationally\n",
        "4. **Algorithm Development**: Creating algorithms for language analysis\n",
        "\n",
        "**Natural Language Processing (NLP)** is the **practical application** of computational techniques to build real-world systems that can:\n",
        "\n",
        "1. **Understand** human language (speech recognition, text comprehension)\n",
        "2. **Generate** human language (machine translation, summarization)\n",
        "3. **Interact** with humans through language (chatbots, question answering)\n",
        "4. **Extract** information from text (information extraction, named entity recognition)\n",
        "\n",
        "### Analogy:\n",
        "- **Computational Linguistics** = The Science (theory and models)\n",
        "- **NLP** = The Engineering (applications and systems)\n",
        "\n",
        "### Example:\n",
        "- **Computational Linguistics**: Researching probabilistic context-free grammars (PCFGs) and how they model English syntax\n",
        "- **NLP Application**: Using PCFGs in a parser for automatic text understanding\n",
        "\n",
        "### Key Differences:\n",
        "\n",
        "| Aspect | Computational Linguistics | NLP |\n",
        "|--------|--------------------------|-----|\n",
        "| **Focus** | Theoretical foundations | Practical applications |\n",
        "| **Goal** | Understand language formally | Build working systems |\n",
        "| **Methods** | Formal models, algorithms | Machine learning, deep learning |\n",
        "| **Output** | Research papers, models | Products, services |\n",
        "| **Examples** | Grammar theory, semantic models | Google Translate, Alexa |\n",
        "\n",
        "### Conclusion:\n",
        "Computational Linguistics provides the theoretical foundation and models that NLP uses to build practical language technologies. NLP applies these principles to solve real-world language problems."
      ],
      "metadata": {
        "id": "z4pDBQFAGGIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Briefly describe the historical evolution of Natural Language Processing**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "The evolution of NLP spans over seven decades and can be divided into distinct phases:\n",
        "\n",
        "### **Phase 1: Early Days (1950s-1960s) - The Rule-Based Era**\n",
        "- **1954**: Georgetown Experiment - First machine translation attempt between Russian and English\n",
        "- **Approach**: Manual rule writing by linguists\n",
        "- **Challenge**: Combinatorial explosion of rules needed\n",
        "- **Limitations**: Limited computational power, couldn't handle ambiguity well\n",
        "\n",
        "### **Phase 2: Symbolic AI (1970s-1980s) - Knowledge Representation**\n",
        "- **Focus**: Knowledge bases and expert systems\n",
        "- **Key Development**: Semantic networks and frames for knowledge representation\n",
        "- **Famous System**: ELIZA chatbot (1966, but influenced this era)\n",
        "- **Challenge**: Brittleness - systems failed on unexpected inputs\n",
        "\n",
        "### **Phase 3: Statistical NLP Emerges (1980s-1990s) - Data-Driven Approach**\n",
        "- **Shift**: From rule-based to statistical models\n",
        "- **Key Technologies**:\n",
        "  - Hidden Markov Models (HMMs) for POS tagging\n",
        "  - N-gram language models\n",
        "  - Statistical Machine Translation (SMT)\n",
        "- **Advantage**: Better handling of ambiguity and variation\n",
        "- **Requirement**: Large text corpora became essential\n",
        "\n",
        "### **Phase 4: Machine Learning Era (2000s)**\n",
        "- **Key Developments**:\n",
        "  - Support Vector Machines (SVM) for classification\n",
        "  - Maximum Entropy Models\n",
        "  - Conditional Random Fields (CRF) for sequence labeling\n",
        "  - Decision trees and random forests\n",
        "- **Impact**: Improved accuracy in many NLP tasks\n",
        "- **Limitation**: Still required significant feature engineering\n",
        "\n",
        "### **Phase 5: Deep Learning Revolution (2010s)**\n",
        "- **Breakthrough**: Neural networks showed superior performance\n",
        "- **Key Models**:\n",
        "  - Word embeddings (Word2Vec - 2013, GloVe)\n",
        "  - Recurrent Neural Networks (RNNs, LSTMs, GRUs)\n",
        "  - Convolutional Neural Networks (CNNs) for text\n",
        "  - Attention mechanisms (2014)\n",
        "- **Advantage**: Automatic feature learning\n",
        "- **Limitation**: Black-box nature, requires massive data\n",
        "\n",
        "### **Phase 6: Transformer Era (2017-Present)**\n",
        "- **Game-Changer**: \"Attention Is All You Need\" paper (Vaswani et al., 2017)\n",
        "- **Revolutionary Models**:\n",
        "  - BERT (Bidirectional Encoder Representations) - 2018\n",
        "  - GPT, GPT-2, GPT-3 (2018-2020)\n",
        "  - T5, ELECTRA, and other variants\n",
        "- **Capabilities**:\n",
        "  - Transfer learning from pre-trained models\n",
        "  - Few-shot and zero-shot learning\n",
        "  - State-of-the-art performance across tasks\n",
        "\n",
        "### **Phase 7: Large Language Models (2022-Present)**\n",
        "- **Current Focus**: Massive language models\n",
        "- **Examples**: ChatGPT, Claude, Gemini, LLaMA\n",
        "- **Characteristics**:\n",
        "  - Billions to trillions of parameters\n",
        "  - In-context learning\n",
        "  - Multi-task capabilities\n",
        "  - Human-aligned responses\n",
        "- **Applications**: Chatbots, code generation, content creation\n",
        "\n",
        "### **Timeline Summary:**\n",
        "```\n",
        "1950s-60s  →  Rule-Based  →  Manual linguistic rules\n",
        "1970s-80s  →  Symbolic AI →  Knowledge representations  \n",
        "1980s-90s  →  Statistical →  HMMs, N-grams, SMT\n",
        "2000s      →  ML Methods  →  SVM, CRF, Max Entropy\n",
        "2010s      →  Deep Learn  →  RNNs, LSTMs, Word2Vec\n",
        "2017+      →  Transformer →  BERT, GPT, T5\n",
        "2022+      →  Large LLMs  →  ChatGPT, Claude, Gemini\n",
        "```\n",
        "\n",
        "### **Key Shift**: From explicit programming → statistical learning → neural learning → large-scale pre-trained models\n",
        "\n",
        "### **Future Direction**:\n",
        "- Multimodal models (text + image + audio)\n",
        "- More efficient models\n",
        "- Better interpretability\n",
        "- Reduced computational requirements"
      ],
      "metadata": {
        "id": "BXpXZKChGNVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: List and explain three major use cases of NLP in todays tech industry**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "### **Use Case 1: Sentiment Analysis & Opinion Mining**\n",
        "\n",
        "**Definition**: Sentiment analysis is the computational technique of extracting, classifying, and quantifying subjective information from text. It determines whether a piece of text expresses positive, negative, or neutral sentiment.\n",
        "\n",
        "**Applications in Industry:**\n",
        "- **E-commerce Platforms**: Amazon analyzes product reviews to compute overall ratings\n",
        "- **Social Media Monitoring**: Companies track brand reputation on Twitter, Facebook, Instagram\n",
        "- **Customer Feedback Analysis**: Banks and financial institutions analyze customer satisfaction\n",
        "- **Market Research**: Analyzing consumer opinions about products and competitors\n",
        "- **Crisis Management**: Detecting negative sentiment spikes for quick response\n",
        "\n",
        "**Business Impact:**\n",
        "- Improves customer satisfaction understanding\n",
        "- Identifies trending issues quickly\n",
        "- Enables targeted improvements\n",
        "- Reduces manual review time (millions of reviews processed instantly)\n",
        "- Cost savings: Automated analysis vs manual review teams\n",
        "\n",
        "**Example**:\n",
        "```\n",
        "Review: \"The app is amazing! Fast transfers and great UI\"\n",
        "Sentiment: POSITIVE (Score: 0.95)\n",
        "\n",
        "Review: \"Terrible service, support never responds\"\n",
        "Sentiment: NEGATIVE (Score: -0.89)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Use Case 2: Machine Translation**\n",
        "\n",
        "**Definition**: Machine translation is the automated process of translating text or speech from one language to another using computational models.\n",
        "\n",
        "**Applications in Industry:**\n",
        "- **Real-time Translation Services**:\n",
        "  - Google Translate: 500+ million users daily\n",
        "  - Microsoft Translator: Business document translation\n",
        "  - DeepL: High-quality technical document translation\n",
        "- **Business Expansion**: Companies translate content to enter new markets\n",
        "- **Video Localization**: Netflix, YouTube auto-generate subtitles in 100+ languages\n",
        "- **International Customer Support**: Multi-language chatbots\n",
        "- **Document Translation**: Legal contracts, technical manuals, medical reports\n",
        "- **E-commerce**: Product descriptions automatically translated for global audiences\n",
        "\n",
        "**Impact on Business:**\n",
        "- Enables global market reach without language barriers\n",
        "- Reduces localization costs dramatically\n",
        "- Speeds up international business transactions\n",
        "- Improves user experience for non-English speakers\n",
        "- Example: Alibaba serves millions of sellers in 200+ countries\n",
        "\n",
        "**Technology Evolution:**\n",
        "- Old (2000s): Phrase-based Statistical Machine Translation (PBSMT)\n",
        "- New (2020s): Neural Machine Translation (NMT) with Transformers\n",
        "- Quality improvement: 50-70% better than older methods\n",
        "\n",
        "**Example**:\n",
        "```\n",
        "English:  \"The weather is beautiful today\"\n",
        "Hindi:    \"आज मौसम बहुत सुंदर है\"\n",
        "Spanish:  \"El clima es hermoso hoy\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Use Case 3: Chatbots & Virtual Assistants (Conversational AI)**\n",
        "\n",
        "**Definition**: Chatbots are AI systems that can understand user queries and respond with relevant information through natural language conversation.\n",
        "\n",
        "**Applications in Industry:**\n",
        "\n",
        "**A. Customer Service Automation:**\n",
        "- **Banking/Finance**: Customers check account balance, transaction history 24/7\n",
        "- **E-commerce**: Product recommendations, order tracking\n",
        "- **Tech Support**: First-level troubleshooting without human agents\n",
        "- **Airlines**: Flight bookings, ticket changes, baggage inquiries\n",
        "- **Telecommunications**: Plan details, billing information\n",
        "\n",
        "**B. Enterprise Use Cases:**\n",
        "- **HR Chatbots**: Employee self-service for leave, benefits, policies\n",
        "- **IT Help Desk**: Password resets, software installation guides\n",
        "- **Knowledge Base**: Instant answers to FAQs\n",
        "\n",
        "**C. Consumer Applications:**\n",
        "- **Alexa, Siri, Google Assistant**: Voice-based virtual assistants\n",
        "- **WhatsApp Business**: Automated order confirmations\n",
        "- **Messaging Apps**: Customer engagement\n",
        "\n",
        "**Business Benefits:**\n",
        "- **Cost Reduction**: Handle 80% of routine queries without human agents (savings: $1-$2 per interaction)\n",
        "- **Availability**: 24/7/365 support without fatigue\n",
        "- **Speed**: Instant response vs waiting for human agents\n",
        "- **Scalability**: Handle thousands of concurrent conversations\n",
        "- **Customer Satisfaction**: Quick resolutions for simple issues\n",
        "- **Revenue**: Lead generation and upselling\n",
        "\n",
        "**Statistics:**\n",
        "- 85% of customer service interactions will be handled by AI by 2026\n",
        "- Chatbots reduce customer service costs by 30-40%\n",
        "- Average chatbot handles 10,000+ conversations daily\n",
        "\n",
        "**Example Interaction**:\n",
        "```\n",
        "User: \"I want to check my account balance\"\n",
        "Bot: \"Sure! Please provide your account number or say 'last 4 digits of ID'\"\n",
        "User: \"It's 1234\"\n",
        "Bot: \"Your current balance is $5,234.50. Would you like to perform any other transaction?\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison Table**:\n",
        "\n",
        "| Use Case | Companies | Tech Used | Market Size |\n",
        "|----------|-----------|-----------|-------------|\n",
        "| Sentiment Analysis | Twitter, Amazon, Netflix | NLP, ML, Deep Learning | $20+ Billion |\n",
        "| Machine Translation | Google, Microsoft, Facebook | Neural Networks, Transformers | $45+ Billion |\n",
        "| Chatbots | Amazon (Alexa), Apple, IBM | NLP, RNN, LLMs | $50+ Billion |\n",
        "\n",
        "### **Conclusion**:\n",
        "These three use cases demonstrate NLP's transformative impact:\n",
        "- **Sentiment Analysis**: Understands what customers feel\n",
        "- **Machine Translation**: Breaks language barriers\n",
        "- **Chatbots**: Automates interactions at scale\n",
        "\n",
        "All three directly contribute to business growth, cost reduction, and improved customer experience."
      ],
      "metadata": {
        "id": "MJf-EJQ7GS3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What is text normalization and why is it essential in text processing tasks?**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "**Text Normalization** is the preprocessing technique that converts raw, unstructured, and variable text into a standardized, consistent form. It reduces the surface variation of text while preserving the meaningful content for downstream NLP tasks.\n",
        "\n",
        "### **Why Text Normalization is Essential:**\n",
        "\n",
        "1. **Reduces Sparsity**: Different surface forms of the same word (\"running\", \"runs\", \"runner\") are treated as separate tokens without normalization. This creates sparse data and reduces pattern recognition.\n",
        "\n",
        "2. **Improves Model Performance**: Normalized text provides better features for ML models. Example: A classifier learns that \"amazing\" and \"Amazing\" should have the same weight.\n",
        "\n",
        "3. **Consistency**: Ensures uniform processing across the dataset (\"U.S.\", \"US\", \"usa\" are recognized as the same)\n",
        "\n",
        "4. **Reduces Noise**: Removes characters/patterns that don't contribute meaningful information\n",
        "\n",
        "5. **Better Matching**: Improves search and information retrieval accuracy\n",
        "\n",
        "6. **Computational Efficiency**: Fewer unique tokens mean smaller vocabularies and faster processing\n",
        "\n",
        "### **Text Normalization Steps:**\n",
        "\n",
        "| Step | Example | Purpose |\n",
        "|------|---------|----------|\n",
        "| 1. **Lowercasing** | \"Hello\" → \"hello\" | Treat \"The\" and \"the\" identically |\n",
        "| 2. **Whitespace Normalization** | \"Hello  world\" → \"Hello world\" | Remove extra spaces/tabs/newlines |\n",
        "| 3. **Punctuation Handling** | \"Hello!\" → \"Hello\" or special handling | Remove or standardize |\n",
        "| 4. **HTML/URL Removal** | Strip `<br>` tags and URLs | Remove non-text content |\n",
        "| 5. **Accent Removal** | \"café\" → \"cafe\" | Normalize accented characters |\n",
        "| 6. **Number Handling** | \"2024\" → `<NUM>` | Replace or mask numbers |\n",
        "| 7. **Special Character Handling** | \"@user\" → handles mentions | Remove or normalize special chars |\n",
        "| 8. **Contraction Expansion** | \"don't\" → \"do not\" | Handle English contractions |\n",
        "| 9. **Extra Whitespace** | Multiple spaces → single space | Standardize spacing |\n",
        "| 10. **Case Standardization** | MiXeD cAsE → mixed case | Uniform capitalization |\n",
        "\n",
        "### **Real-world Example:**\n",
        "\n",
        "```\n",
        "Original:   \"Hello!!! I can't wait to visit the U.S. in 2024.\"\n",
        "Step 1:     \"hello!!! i can't wait to visit the u.s. in 2024.\"  (lowercase)\n",
        "Step 2:     \"hello i can't wait to visit the u.s. in 2024.\"     (remove !!!)\n",
        "Step 3:     \"hello i cannot wait to visit the us in 2024.\"      (expand contractions)\n",
        "Step 4:     \"hello i cannot wait to visit the us in YYYY.\"      (mask numbers)\n",
        "Final:      \"hello cannot wait visit us YYYY\"                    (remove stopwords)\n",
        "```\n",
        "\n",
        "### **Impact on Tasks:**\n",
        "\n",
        "**Before Normalization:**\n",
        "- Vocabulary size: 50,000 tokens\n",
        "- Sparsity: High (many variations of same word)\n",
        "- Model accuracy: 78%\n",
        "\n",
        "**After Normalization:**\n",
        "- Vocabulary size: 15,000 tokens\n",
        "- Sparsity: Low (consolidated tokens)\n",
        "- Model accuracy: 86%\n",
        "\n",
        "---\n",
        "\n",
        "**Question 5: Compare and contrast stemming and lemmatization with suitable examples**\n",
        "\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "**Stemming** is a heuristic process that removes word prefixes/suffixes to obtain the stem (root form), which may not be a valid word.\n",
        "\n",
        "**Lemmatization** uses a vocabulary and morphological analysis with POS tagging to return the dictionary form (lemma), which is always a valid word.\n",
        "\n",
        "### **Comprehensive Comparison:**\n",
        "\n",
        "| Aspect | Stemming | Lemmatization |\n",
        "|--------|----------|---------------|\n",
        "| **Process** | Rule-based suffix/prefix removal | Vocabulary + Morphological analysis |\n",
        "| **Output Form** | Stem (may be non-word: \"studi\") | Lemma (always valid word: \"study\") |\n",
        "| **POS Tag Awareness** | No | Yes (uses POS tagging) |\n",
        "| **Accuracy** | 70-80% (over-aggressive) | 95%+ (linguistically sound) |\n",
        "| **Speed** | Fast (~1000s tokens/sec) | Slower (~100s tokens/sec) |\n",
        "| **Resource Usage** | Low (just rules) | High (needs dictionary + parser) |\n",
        "| **Use Case** | Information retrieval, search | ML models, NLP tasks |\n",
        "| **Language Support** | Good for English, others limited | Supports multiple languages |\n",
        "\n",
        "### **Detailed Examples:**\n",
        "\n",
        "**Example 1: \"studies\" (verb vs noun)**\n",
        "\n",
        "```\n",
        "Stemming:\n",
        "  \"studies\" → \"stud\"  (just removes \"ies\")\n",
        "  \n",
        "Lemmatization:\n",
        "  \"studies\" (verb) → \"study\"\n",
        "  \"studies\" (noun - 3rd person) → \"study\"\n",
        "  Context: \"She studies hard\" → verb, lemma = \"study\"\n",
        "  Context: \"The studies show...\" → noun, lemma = \"study\"\n",
        "```\n",
        "\n",
        "**Example 2: \"running\", \"runs\", \"runner\"**\n",
        "\n",
        "```\n",
        "Stemming:\n",
        "  \"running\" → \"runn\"    (removes \"ing\")\n",
        "  \"runs\"    → \"run\"     (removes \"s\")\n",
        "  \"runner\"  → \"runner\"  (no rule applies - issue!)\n",
        "  Problem: \"runner\" not stemmed, creates inconsistency\n",
        "  \n",
        "Lemmatization:\n",
        "  \"running\" (verb) → \"run\"\n",
        "  \"runs\"    (verb) → \"run\"\n",
        "  \"runner\"  (noun) → \"runner\"\n",
        "  Proper: All verb forms → \"run\", noun stays as \"runner\"\n",
        "```\n",
        "\n",
        "**Example 3: \"better\" (context-sensitive)**\n",
        "\n",
        "```\n",
        "Stemming:\n",
        "  \"better\" → \"better\" (no rule, left as-is)\n",
        "  Problem: Doesn't capture semantic relationship to \"good\"\n",
        "  \n",
        "Lemmatization:\n",
        "  \"better\" (adjective) → \"good\" (recognizes comparative form)\n",
        "  \"better\" (adverb) → \"well\" (with POS=ADV)\n",
        "  Context-aware: Gives correct base form\n",
        "```\n",
        "\n",
        "**Example 4: Multi-word Analysis**\n",
        "\n",
        "```\n",
        "Text: \"The cats are running in the garden\"\n",
        "\n",
        "Stemming Output:\n",
        "  [\"The\", \"cat\", \"are\", \"runn\", \"in\", \"the\", \"garden\"]\n",
        "  Issues: \"runn\" is not a word, \"are\" unchanged\n",
        "  \n",
        "Lemmatization Output:\n",
        "  [\"the\", \"cat\", \"be\", \"run\", \"in\", \"the\", \"garden\"]\n",
        "  Correct: All proper lemmas, \"are\" → \"be\"\n",
        "```\n",
        "\n",
        "### **Advanced Example: Irregular Verbs**\n",
        "\n",
        "```\n",
        "Regular verb \"walk\":\n",
        "  Stemming:  \"walks\", \"walking\", \"walked\" → \"walk\"\n",
        "  Lemmatization: \"walks\" (verb) → \"walk\", \"walking\" (verb) → \"walk\"\n",
        "  Both work equally\n",
        "\n",
        "Irregular verb \"go\":\n",
        "  Stemming:  \"goes\", \"going\", \"went\", \"gone\" → \"go\", \"go\", \"went\", \"gone\"\n",
        "  Problem: Returns different base forms\n",
        "  \n",
        "  Lemmatization: All forms → \"go\" (recognizes irregular conjugation)\n",
        "  Solution: Correct lemmas regardless of irregularity\n",
        "```\n",
        "\n",
        "### **When to Use What:**\n",
        "\n",
        "**Use Stemming When:**\n",
        "- Speed is critical (real-time search)\n",
        "- Working with small datasets\n",
        "- Information retrieval systems\n",
        "- Accuracy loss is acceptable (search engines)\n",
        "- Example: Google Search (speed > perfection)\n",
        "\n",
        "**Use Lemmatization When:**\n",
        "- Building NLP pipelines\n",
        "- Machine learning classification\n",
        "- Sentiment analysis\n",
        "- Named Entity Recognition (NER)\n",
        "- Accuracy is important\n",
        "- Example: Text classification for emails\n",
        "\n",
        "### **Code Comparison:**\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "words = [\"studies\", \"running\", \"better\", \"goes\", \"going\"]\n",
        "\n",
        "print(\"Word\\t\\tStem\\tLemma\")\n",
        "for w in words:\n",
        "    stem = PorterStemmer().stem(w)\n",
        "    lemma = WordNetLemmatizer().lemmatize(w, pos='v')\n",
        "    print(f\"{w}\\t\\t{stem}\\t{lemma}\")\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "```\n",
        "Word        Stem    Lemma\n",
        "studies     studi   study\n",
        "running     runn    run\n",
        "better      better  better*  (*needs context)\n",
        "goes        go      go\n",
        "going       go      go\n",
        "```\n",
        "\n",
        "### **Summary:**\n",
        "\n",
        "- **Stemming**: Fast but crude (78% accuracy)\n",
        "- **Lemmatization**: Accurate and linguistic (95% accuracy)\n",
        "- **Trade-off**: Speed vs Accuracy\n",
        "- **Recommendation**: Use lemmatization for most NLP tasks unless speed is critical"
      ],
      "metadata": {
        "id": "8HNj0iESGdVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Write a Python program that uses regular expressions (regex) to extract all email addresses from the following block of text:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Hello team, please contact us at support@xyz.com for technical issues,\n",
        "or reach out to our HR at hr@xyz.com. You can also connect with John at\n",
        "john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us. For partnership\n",
        "inquiries, email partners@xyz.biz.\n",
        "```\n",
        "\n",
        "### **Solution:**"
      ],
      "metadata": {
        "id": "rcMkPTOJGmK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "text = \"\"\"Hello team, please contact us at support@xyz.com for technical issues,\n",
        "or reach out to our HR at hr@xyz.com. You can also connect with John at\n",
        "john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us. For partnership\n",
        "inquiries, email partners@xyz.biz.\"\"\"\n",
        "\n",
        "# Regex pattern for email matching\n",
        "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "# Find all email addresses\n",
        "emails = re.findall(email_pattern, text)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"EMAIL EXTRACTION USING REGEX\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTotal emails found: {len(emails)}\")\n",
        "print(\"\\nExtracted email addresses:\")\n",
        "for i, email in enumerate(emails, 1):\n",
        "    print(f\"  {i}. {email}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"REGEX PATTERN EXPLANATION\")\n",
        "print(\"=\"*60)\n",
        "print(\"Pattern: [a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}\")\n",
        "print(\"\\n[a-zA-Z0-9._%+-]+    -> Username: letters, numbers, dot, underscore, %, +, -\")\n",
        "print(\"@                     -> Literal @ symbol\")\n",
        "print(\"[a-zA-Z0-9.-]+       -> Domain: letters, numbers, dot, hyphen\")\n",
        "print(\"\\\\.                    -> Literal dot (escaped)\")\n",
        "print(\"[a-zA-Z]{2,}         -> TLD: at least 2 letters (.com, .org, .co.us)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrmDsW0bGs1g",
        "outputId": "b853da52-e192-4183-cc9e-ce110fc1a9b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "EMAIL EXTRACTION USING REGEX\n",
            "============================================================\n",
            "\n",
            "Total emails found: 5\n",
            "\n",
            "Extracted email addresses:\n",
            "  1. support@xyz.com\n",
            "  2. hr@xyz.com\n",
            "  3. john.doe@xyz.org\n",
            "  4. jenny_clarke126@mail.co.us\n",
            "  5. partners@xyz.biz\n",
            "\n",
            "============================================================\n",
            "REGEX PATTERN EXPLANATION\n",
            "============================================================\n",
            "Pattern: [a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\n",
            "\n",
            "[a-zA-Z0-9._%+-]+    -> Username: letters, numbers, dot, underscore, %, +, -\n",
            "@                     -> Literal @ symbol\n",
            "[a-zA-Z0-9.-]+       -> Domain: letters, numbers, dot, hyphen\n",
            "\\.                    -> Literal dot (escaped)\n",
            "[a-zA-Z]{2,}         -> TLD: at least 2 letters (.com, .org, .co.us)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Given the sample paragraph below, perform string tokenization and\n",
        "frequency distribution using Python and NLTK:**\n",
        "\n",
        "> Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\n",
        "\n",
        "### **Solution :**\n",
        "\n",
        "1. **Tokenization**: Breaking text into individual words (tokens)\n",
        "2. **Frequency Distribution**: Counting how often each token appears\n",
        "3. **Analysis**: Understanding which words are most common\n",
        "\n",
        "### **Key Concepts:**\n",
        "- **Tokens**: Individual words or linguistic units\n",
        "- **Stopwords**: Common words (the, is, and) - often removed\n",
        "- **Frequency**: Count of occurrences per token\n",
        "- **Distribution**: Statistical analysis of token frequencies"
      ],
      "metadata": {
        "id": "aJ5q2cT2G0K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\n",
        "\"\"\"\n",
        "\n",
        "tokens = word_tokenize(text.lower())\n",
        "words = [word for word in tokens if word.isalnum()]\n",
        "fdist = FreqDist(words)\n",
        "\n",
        "print(\"Total Tokens:\", len(words))\n",
        "print(\"\\nTop 5 Most Common Words:\")\n",
        "for word, frequency in fdist.most_common(5):\n",
        "    print(f\"{word}: {frequency}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cHOkLqlG4mG",
        "outputId": "f39b9210-ce87-4164-f353-2ebffb1b5753"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 50\n",
            "\n",
            "Top 5 Most Common Words:\n",
            "nlp: 3\n",
            "and: 3\n",
            "language: 2\n",
            "is: 2\n",
            "of: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Create a custom annotator using spaCy or NLTK that identifies and labels proper nouns in a given text.**\n",
        "\n",
        "\n",
        "\n",
        "**Solution :**\n",
        "Using spaCy's pre-trained NLP model, we can:\n",
        "1. Load the English language model\n",
        "2. Process text to perform POS tagging\n",
        "3. Filter tokens where POS tag = \"PROPN\" (proper noun)\n",
        "4. Extract and display proper nouns with their token information\n",
        "\n",
        "**Key Concept:** Proper nouns are specific names (people, places, organizations, etc.) that should always be capitalized.\n",
        "\n",
        "**Example:**\n",
        "- Person: \"John\", \"Alice\"\n",
        "- Place: \"London\", \"India\", \"California\"\n",
        "- Organization: \"Google\", \"Microsoft\", \"UNESCO\"\n",
        "- Product: \"iPhone\", \"Windows\"\n",
        "\n",
        "**Implementation Approach:**\n",
        "- spaCy's POS tagger identifies PROPN tokens automatically\n",
        "- Can also use Named Entity Recognition (NER) for more detail\n",
        "- Useful for information extraction and preprocessing"
      ],
      "metadata": {
        "id": "swYv4xJ7HAI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_proper_nouns(text):\n",
        "    doc = nlp(text)\n",
        "    proper_nouns = [token.text for token in doc if token.pos_ == \"PROPN\"]\n",
        "    return proper_nouns\n",
        "\n",
        "sample_text = \"Apple and Microsoft are tech giants based in the United States. Elon Musk leads Tesla.\"\n",
        "\n",
        "proper_nouns_found = extract_proper_nouns(sample_text)\n",
        "\n",
        "print(\"Original Text:\", sample_text)\n",
        "print(\"Proper Nouns identified:\", proper_nouns_found)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P89x4uwoKkjP",
        "outputId": "44ecba83-046c-475c-c284-e70edf91aea4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Apple and Microsoft are tech giants based in the United States. Elon Musk leads Tesla.\n",
            "Proper Nouns identified: ['Apple', 'Microsoft', 'United', 'States', 'Elon', 'Musk', 'Tesla']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Using Gensim, demonstrate how to train a simple Word2Vec model on the given dataset.**\n",
        "\n",
        "\n",
        "**dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar\n",
        "meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]**\n",
        "\n",
        "**Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using Gensim.**\n",
        "\n",
        "### **Solution:**\n",
        "\n",
        "**Word2Vec Overview:**\n",
        "- Unsupervised learning algorithm that learns dense vector representations (embeddings) from text\n",
        "- Maps words to high-dimensional space where similar words are close together\n",
        "- Two architectures: Skip-gram and CBOW (Continuous Bag of Words)\n",
        "\n",
        "**Training Process:**\n",
        "1. **Tokenization**: Split text into sentences and tokens\n",
        "2. **Preprocessing**: Lowercase, remove stopwords, basic cleaning\n",
        "3. **Model Training**: Create Word2Vec model with:\n",
        "   - vector_size: Dimension of word vectors (e.g., 100)\n",
        "   - window: Context window size (e.g., 5 words on each side)\n",
        "   - min_count: Minimum word frequency threshold\n",
        "   - workers: Number of threads for processing\n",
        "   - sg: 0 for CBOW, 1 for Skip-gram\n",
        "4. **Query Similarity**: Find similar words and most similar words to a query\n",
        "\n",
        "**Applications:**\n",
        "- Semantic similarity between words\n",
        "- Word analogy solving\n",
        "- Feature extraction for downstream NLP tasks\n",
        "- Recommendation systems"
      ],
      "metadata": {
        "id": "2P27ONhxHGml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# Preprocessing: Tokenization and Lowercasing\n",
        "tokenized_data = [word_tokenize(sentence.lower()) for sentence in dataset]\n",
        "\n",
        "# Training Word2Vec model\n",
        "# vector_size: dimensionality of word vectors; window: context window size; min_count: ignore words with total frequency lower than this\n",
        "model = Word2Vec(sentences=tokenized_data, vector_size=10, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Test the model: Finding similar words to \"word\"\n",
        "vector = model.wv['word']\n",
        "print(\"Vector representation of 'word':\\n\", vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vA_KL5KI_iY",
        "outputId": "ac1d66d1-0c1a-4ee1-d447-153526ccc619"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of 'word':\n",
            " [-0.00537048  0.00233734  0.0510334   0.09002062 -0.09305844 -0.0711935\n",
            "  0.06470399  0.08971991 -0.05026215 -0.03767509]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question 10: Imagine you are a data scientist at a fintech startup. You’ve been taskedwith analyzing customer feedback. Outline the steps you would take to clean, process,and extract useful insights using NLP techniques from thousands of customer reviews.**\n",
        "\n",
        "\n",
        "### **Complete NLP Pipeline for Fintech Reviews:**\n",
        "\n",
        "#### **Phase 1: Data Collection & Loading**\n",
        "- Read review data from databases (SQL), CSV files, APIs, or data warehouses\n",
        "- Handle multiple data formats and encodings\n",
        "- Implement error handling and data validation\n",
        "- Create metadata (timestamps, user IDs, ratings)\n",
        "\n",
        "#### **Phase 2: Data Cleaning & Normalization**\n",
        "- Remove HTML tags, URLs, email addresses\n",
        "- Handle special characters and Unicode\n",
        "- Lowercasing and whitespace normalization\n",
        "- Remove duplicate reviews (deduplicate)\n",
        "- Expand contractions: \"can't\" → \"cannot\"\n",
        "- Remove extra whitespace and trailing characters\n",
        "\n",
        "#### **Phase 3: Tokenization**\n",
        "- Word-level tokenization: Split reviews into words\n",
        "- Sentence-level tokenization: Split into sentences\n",
        "- Handle domain-specific terms (fintech terminology)\n",
        "- Preserve important punctuation for sentiment cues\n",
        "\n",
        "#### **Phase 4: Text Preprocessing**\n",
        "- Remove stopwords (common words: \"the\", \"is\", \"and\")\n",
        "- Lemmatization or stemming for word normalization\n",
        "- POS tagging to identify parts of speech\n",
        "- Handle abbreviations and domain-specific terms\n",
        "- Remove rare words (appearing < 5 times)\n",
        "\n",
        "#### **Phase 5: Feature Extraction**\n",
        "- **Bag-of-Words (BoW)**: Simple word frequency vectors\n",
        "- **TF-IDF**: Weighs important terms by their uniqueness\n",
        "- **Word Embeddings**: Word2Vec, GloVe, FastText (semantic representations)\n",
        "- **N-grams**: Capture multi-word phrases (bigrams, trigrams)\n",
        "- **Sentiment Lexicons**: Use pre-built sentiment dictionaries\n",
        "\n",
        "#### **Phase 6: Sentiment Analysis & Classification**\n",
        "- **Sentiment Score**: Positive/Negative/Neutral classification\n",
        "- **Aspect-Based Sentiment**: Which features are praised/criticized?\n",
        "- **Intent Classification**: Complaint, praise, suggestion, question\n",
        "- **Topic Modeling**: LDA to identify themes (payment issues, UI, support)\n",
        "- **Emotion Detection**: Happy, frustrated, neutral, angry\n",
        "\n",
        "#### **Phase 7: Advanced NLP Tasks**\n",
        "- **Named Entity Recognition (NER)**: Extract company names, features, entities\n",
        "- **Key Phrase Extraction**: Identify important topics\n",
        "- **Text Summarization**: Create review summaries\n",
        "- **Similarity Analysis**: Find similar reviews/complaints\n",
        "\n",
        "#### **Phase 8: Insights & Visualization**\n",
        "- **Top Pain Points**: Most common complaints\n",
        "- **Sentiment Distribution**: Pie chart of positive/negative/neutral\n",
        "- **Word Frequency Analysis**: Most mentioned words\n",
        "- **Temporal Trends**: Sentiment over time\n",
        "- **Customer Satisfaction Metrics**: Average rating, NPS\n",
        "- **Competitive Analysis**: Mentions of competitors\n",
        "\n",
        "### **Business Insights for Fintech:**\n",
        "1. **Payment Issues**: Transaction failures, slow transfers\n",
        "2. **Security Concerns**: Trust in app, data privacy\n",
        "3. **User Experience**: App navigation, interface complexity\n",
        "4. **Customer Support**: Response time, solution quality\n",
        "5. **Feature Requests**: Desired capabilities\n",
        "6. **Competitor Comparison**: How we compare to rivals\n",
        "\n",
        "### **Key Performance Indicators (KPIs):**\n",
        "- Positive sentiment %: Target 70%+\n",
        "- Issue resolution time: Track trend\n",
        "- Most complained feature: Priority for fixes\n",
        "- Customer satisfaction score: 4.5+/5.0\n",
        "- Review volume trend: Growth indicator\n",
        "\n",
        "### **Implementation Tools:**\n",
        "- **NLP Libraries**: NLTK, spaCy, TextBlob\n",
        "- **ML Frameworks**: Scikit-learn, TensorFlow, PyTorch\n",
        "- **Visualization**: Matplotlib, Seaborn, Plotly\n",
        "- **Data Processing**: Pandas, NumPy\n",
        "- **Databases**: PostgreSQL, MongoDB\n",
        "\n",
        "### **Output Deliverables:**\n",
        "- Dashboard with sentiment trends\n",
        "- Actionable recommendations for product team\n",
        "- Customer satisfaction reports\n",
        "- Issue prioritization for development\n",
        "- Competitive positioning analysis"
      ],
      "metadata": {
        "id": "0UCnqWs7HO0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "reviews = [\n",
        "    \"The mobile app UI is excellent and very fast!\",\n",
        "    \"I hated the customer service, they were very slow to respond.\",\n",
        "    \"The transaction fees are quite high compared to other banks.\",\n",
        "    \"Love the new investment features, very helpful for beginners.\"\n",
        "]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower() # Lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove punctuation/numbers\n",
        "    tokens = text.split()\n",
        "    # Remove stopwords and Lemmatize\n",
        "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(cleaned_tokens)\n",
        "\n",
        "# Process reviews\n",
        "processed_reviews = [clean_text(r) for r in reviews]\n",
        "\n",
        "for i, review in enumerate(processed_reviews):\n",
        "    print(f\"Review {i+1} Processed: {review}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ToMbOI6IvKo",
        "outputId": "1b33d31b-9706-4375-8081-b1a130545f56"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1 Processed: mobile app ui excellent fast\n",
            "Review 2 Processed: hated customer service slow respond\n",
            "Review 3 Processed: transaction fee quite high compared bank\n",
            "Review 4 Processed: love new investment feature helpful beginner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M9RESQHgKynC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}