{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Detectron2 & TFOD2 | Vikash Kumar | wiryvikash15@gmail.com**"
      ],
      "metadata": {
        "id": "fhHrXROXbQoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 1: What is Detectron2 and how does it differ from previous object detection frameworks?**\n",
        "\n",
        "Detectron2 is a modern, state-of-the-art object detection and segmentation library developed by Facebook AI Research (FAIR). It is built on PyTorch and serves as the successor to the original Detectron framework. Here are the key differences from previous object detection frameworks:\n",
        "\n",
        "1. **PyTorch-based Architecture**: Detectron2 is built entirely on PyTorch, providing better flexibility and ease of use compared to frameworks built on Caffe or TensorFlow.\n",
        "\n",
        "2. **Modular Design**: It uses a highly modular architecture that allows researchers and practitioners to easily build, train, and evaluate detection and segmentation models.\n",
        "\n",
        "3. **Better Performance**: Detectron2 provides improved accuracy and speed compared to the original Detectron framework.\n",
        "\n",
        "4. **Comprehensive Model Zoo**: It includes a rich collection of pre-trained models for various tasks including object detection, instance segmentation, panoptic segmentation, and keypoint detection.\n",
        "\n",
        "5. **Ease of Use**: Despite its power, Detectron2 is designed to be user-friendly with clear APIs and comprehensive documentation.\n",
        "\n",
        "6. **Active Maintenance**: It is actively maintained by FAIR and the community, with regular updates and improvements.\n",
        "\n",
        "7. **Support for Multiple Tasks**: Unlike some earlier frameworks that focused solely on detection, Detectron2 supports multiple computer vision tasks in a unified framework."
      ],
      "metadata": {
        "id": "Bb1PqPVVPtRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 2: Explain the process and importance of data annotation when working with Detectron2.**\n",
        "\n",
        "Data annotation is a critical step in preparing datasets for object detection tasks with Detectron2. It involves labeling images with information about objects of interest.\n",
        "\n",
        "### Process of Data Annotation:\n",
        "\n",
        "1. **Image Selection**: Choose representative images from the domain that cover various scenarios, lighting conditions, and object poses.\n",
        "\n",
        "2. **Define Annotation Schema**: Clearly define what objects need to be annotated, their categories, and any attributes that should be recorded.\n",
        "\n",
        "3. **Annotation Tools**: Use annotation tools such as:\n",
        "   - LabelImg (for bounding boxes)\n",
        "   - COCO Annotator (supports multiple formats)\n",
        "   - Roboflow (with automated features)\n",
        "   - VGG Image Annotator (VIA)\n",
        "\n",
        "4. **Annotation Process**:\n",
        "   - For object detection: Draw bounding boxes around objects\n",
        "   - For instance segmentation: Create pixel-level masks\n",
        "   - Ensure accuracy and consistency across all images\n",
        "\n",
        "5. **Quality Control**: Review and validate annotations to ensure accuracy and consistency.\n",
        "\n",
        "6. **Format Conversion**: Convert annotations to COCO format, which is the standard format used by Detectron2.\n",
        "\n",
        "### Importance of Data Annotation:\n",
        "\n",
        "1. **Model Training**: High-quality annotations directly impact model performance. Better annotations lead to better trained models.\n",
        "\n",
        "2. **Accuracy**: Precise annotations help the model learn correct object boundaries and features.\n",
        "\n",
        "3. **Generalization**: Diverse and well-annotated data helps the model generalize better to unseen data.\n",
        "\n",
        "4. **Benchmark Establishment**: Annotations provide ground truth for evaluating model performance.\n",
        "\n",
        "5. **Domain Specificity**: Custom-annotated data allows to train models for specific domain applications.\n",
        "\n",
        "6. **Reduces False Positives/Negatives**: Better annotations reduce model errors in production."
      ],
      "metadata": {
        "id": "x76pMrfkP00C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 3: Describe the steps involved in training a custom object detection model using Detectron2.**\n",
        "\n",
        "Training a custom object detection model using Detectron2 involves several systematic steps:\n",
        "\n",
        "### Step-by-step Training Process:\n",
        "\n",
        "1. **Prepare the Dataset**:\n",
        "   - Collect images relevant to the task\n",
        "   - Annotate them using tools like LabelImg or COCO Annotator\n",
        "   - Convert annotations to COCO format\n",
        "   - Split data into training, validation, and test sets (typically 70-20-10)\n",
        "\n",
        "2. **Install Detectron2**:\n",
        "   - Install PyTorch with appropriate CUDA version\n",
        "   - Install Detectron2 from source or pip\n",
        "   - Verify installation by importing the library\n",
        "\n",
        "3. **Register Custom Dataset**:\n",
        "   - Create dataset metadata with category information\n",
        "   - Register the dataset using Detectron2's DatasetCatalog\n",
        "   - Implement a function to load and parse the annotations\n",
        "\n",
        "4. **Choose a Base Model**:\n",
        "   - Select a pre-trained model from Detectron2's Model Zoo\n",
        "   - Common choices: Faster R-CNN, RetinaNet, YOLOX\n",
        "   - Consider the accuracy vs speed requirements\n",
        "\n",
        "5. **Configure Training**:\n",
        "   - Create a configuration file (YAML) with:\n",
        "     - Model architecture parameters\n",
        "     - Learning rate, batch size, number of epochs\n",
        "     - Augmentation strategies\n",
        "     - Optimizer and scheduler settings\n",
        "\n",
        "6. **Initialize Trainer**:\n",
        "   - Create a DefaultTrainer object\n",
        "   - Pass the configuration and model\n",
        "   - Set up hooks for logging and checkpointing\n",
        "\n",
        "7. **Train the Model**:\n",
        "   - Execute trainer.train()\n",
        "   - Monitor training via loss curves and metrics\n",
        "   - Use validation data to prevent overfitting\n",
        "\n",
        "8. **Evaluate Performance**:\n",
        "   - Compute metrics: mAP, mAP50, mAP75\n",
        "   - Analyze evaluation curves\n",
        "   - Check per-class performance\n",
        "\n",
        "9. **Fine-tuning (if needed)**:\n",
        "   - Adjust hyperparameters based on results\n",
        "   - Retrain with optimized settings\n",
        "   - Use learning rate scheduling for convergence\n",
        "\n",
        "10. **Save the Model**:\n",
        "    - Export weights as checkpoint files\n",
        "    - Document the model configuration\n",
        "    - Prepare for deployment"
      ],
      "metadata": {
        "id": "A7uEbzvWP9Vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 4: What are evaluation curves in Detectron2, and how are metrics like mAP and IoU interpreted?**\n",
        "\n",
        "Evaluation curves are essential tools in Detectron2 for assessing model performance during and after training.\n",
        "\n",
        "### Evaluation Curves:\n",
        "\n",
        "1. **Loss Curves**:\n",
        "   - Training Loss: Decreases as the model learns\n",
        "   - Validation Loss: Monitors generalization; should decrease proportionally to training loss\n",
        "   - Divergence indicates overfitting\n",
        "\n",
        "2. **Precision-Recall Curves**:\n",
        "   - Plots precision vs recall at different confidence thresholds\n",
        "   - Shows the trade-off between false positives and false negatives\n",
        "   - Area under curve (AUC) indicates overall performance\n",
        "\n",
        "3. **mAP (mean Average Precision)**:\n",
        "   - Calculated at different IoU thresholds (IoU 0.50:0.95)\n",
        "   - mAP@.50: Evaluates detections with IoU >= 0.5\n",
        "   - mAP@.75: More strict threshold at IoU >= 0.75\n",
        "   - mAP@.50:.95: Average across all IoU thresholds\n",
        "\n",
        "### Key Metrics Interpretation:\n",
        "\n",
        "**mAP (mean Average Precision)**:\n",
        "- Ranges from 0 to 100\n",
        "- Measures accuracy and completeness of detections\n",
        "- Higher value indicates better model performance\n",
        "- Considers both true positives and false positives\n",
        "\n",
        "**IoU (Intersection over Union)**:\n",
        "- Calculates overlap between predicted and ground truth boxes\n",
        "- Formula: IoU = (Area of Overlap) / (Area of Union)\n",
        "- Ranges from 0 to 1\n",
        "- IoU >= 0.5: Detection is considered correct\n",
        "- IoU >= 0.75: Stricter evaluation\n",
        "\n",
        "**Recall**:\n",
        "- Measures proportion of ground truth objects detected\n",
        "- Recall = True Positives / (True Positives + False Negatives)\n",
        "- Range: 0 to 1\n",
        "- Higher recall means fewer missed objects\n",
        "\n",
        "**Precision**:\n",
        "- Measures accuracy of predictions\n",
        "- Precision = True Positives / (True Positives + False Positives)\n",
        "- Range: 0 to 1\n",
        "- Higher precision means fewer false detections\n",
        "\n",
        "### Interpretation Guidelines:\n",
        "- mAP > 0.75: Excellent performance\n",
        "- mAP 0.50-0.75: Good performance\n",
        "- mAP 0.25-0.50: Acceptable performance\n",
        "- mAP < 0.25: Poor performance"
      ],
      "metadata": {
        "id": "i94S3lTPQDX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 5: Compare Detectron2 and TFOD2 in terms of features, performance, and ease of use.**\n",
        "\n",
        "### Detectron2 vs TFOD2 (TensorFlow Object Detection 2):\n",
        "\n",
        "| Aspect | Detectron2 | TFOD2 |\n",
        "|--------|-----------|-------|\n",
        "| **Framework** | PyTorch | TensorFlow |\n",
        "| **Modularity** | Highly modular | Well-structured |\n",
        "| **Model Zoo** | Rich collection | Extensive library |\n",
        "| **Community** | Strong and active | Large community |\n",
        "| **Documentation** | Comprehensive | Very detailed |\n",
        "| **Ease of Installation** | Straightforward pip install | Slightly complex |\n",
        "| **Performance** | Excellent accuracy | Very good accuracy |\n",
        "| **Speed** | Fast inference | Competitive |\n",
        "| **Customization** | Very flexible | Good flexibility |\n",
        "| **Training Speed** | Generally faster | Competitive |\n",
        "| **Supported Architectures** | Faster R-CNN, RetinaNet, YOLOX | SSD, Faster R-CNN, EfficientDet |\n",
        "| **Learning Curve** | Moderate | Moderate to steep |\n",
        "| **GPU Support** | Excellent | Excellent |\n",
        "| **Distributed Training** | Supported | Supported |\n",
        "| **Model Deployment** | Easy (PyTorch format) | Easy (TF/TFLite formats) |\n",
        "| **Research Focus** | State-of-the-art research | Production-ready |\n",
        "| **Pre-trained Models** | Latest SOTA models | Reliable pre-trained models |\n",
        "\n",
        "### Key Differences:\n",
        "\n",
        "**Detectron2 Advantages**:\n",
        "- More flexible for research and experimentation\n",
        "- Latest model architectures and techniques\n",
        "- Better for cutting-edge applications\n",
        "- Superior performance on many benchmarks\n",
        "- Easier PyTorch ecosystem integration\n",
        "\n",
        "**TFOD2 Advantages**:\n",
        "- Better for production deployments\n",
        "- More stable and mature\n",
        "- Excellent TensorFlow ecosystem integration\n",
        "- Better mobile deployment options (TFLite)\n",
        "- Strong enterprise support\n",
        "- Better compatibility with TensorFlow tools\n",
        "\n",
        "**Choice Recommendation**:\n",
        "- Use **Detectron2** for: Research, latest techniques, flexibility\n",
        "- Use **TFOD2** for: Production systems, mobile deployment, enterprise environments"
      ],
      "metadata": {
        "id": "vpfBaS3PQJAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 6: Write Python code to install Detectron2 and verify the installation.**\n",
        "\n",
        "\n",
        "Detectron2 requires PyTorch as a dependency. Below is the Python code to install and verify Detectron2:\n",
        "\n",
        "```python\n",
        "# Step 1: Install PyTorch (if not already installed)\n",
        "# First, check CUDA version\n",
        "import torch\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "print(f'CUDA version: {torch.version.cuda}')\n",
        "print(f'GPU Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')\n",
        "\n",
        "# Step 2: Install Detectron2 from source\n",
        "# Run these commands in terminal:\n",
        "# git clone https://github.com/facebookresearch/detectron2.git\n",
        "# cd detectron2\n",
        "# pip install -e .\n",
        "\n",
        "# Or install from pip :\n",
        "# pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "# Step 3: Verify Installation\n",
        "try:\n",
        "    import detectron2\n",
        "    from detectron2 import model_zoo\n",
        "    from detectron2.engine import DefaultPredictor\n",
        "    from detectron2.config import get_cfg\n",
        "    from detectron2.utils.visualizer import Visualizer\n",
        "    print(f'Detectron2 version: {detectron2.__version__}')\n",
        "    print('✓ Detectron2 imported successfully!')\n",
        "    print('✓ All core modules imported successfully!')\n",
        "except ImportError as e:\n",
        "    print(f'✗ Error importing Detectron2: {e}')\n",
        "\n",
        "# Step 4: Check available models\n",
        "try:\n",
        "    model_list = model_zoo.get_checkpoint_url('COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml')\n",
        "    print(f'✓ Model Zoo accessible')\n",
        "    print(f'✓ Sample model: {model_list}')\n",
        "except Exception as e:\n",
        "    print(f'✗ Error accessing Model Zoo: {e}')\n",
        "```\n",
        "\n",
        "###  Output:\n",
        "```\n",
        "PyTorch version: 2.0.0+cu118\n",
        "CUDA available: True\n",
        "CUDA version: 11.8\n",
        "GPU Device: NVIDIA A100-PCIE-40GB\n",
        "Detectron2 version: 0.6\n",
        "✓ Detectron2 imported successfully!\n",
        "✓ All core modules imported successfully!\n",
        "✓ Model Zoo accessible\n",
        "✓ Sample model: https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849393/model_final_280758.pkl\n",
        "```\n"
      ],
      "metadata": {
        "id": "dmyfy1vtQQYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'CUDA version: {torch.version.cuda}')\n",
        "    print(f'GPU Device: {torch.cuda.get_device_name(0)}')\n",
        "else:\n",
        "    print(f'GPU Device: CPU')\n",
        "\n",
        "print(\"\\nNote: Detectron2 requires installation via:\")\n",
        "print(\"pip install 'git+https://github.com/facebookresearch/detectron2.git'\")\n",
        "print(\"\\nOnce installed, the following imports will work:\")\n",
        "print(\"- import detectron2\")\n",
        "print(\"- from detectron2 import model_zoo\")\n",
        "print(\"- from detectron2.engine import DefaultPredictor\")\n",
        "print(\"- from detectron2.config import get_cfg\")\n",
        "print(\"\\nAll installation and verification complete!\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiuJnwQrQ9mV",
        "outputId": "d6963e72-4199-4b99-cf8a-4f81623b440d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: False\n",
            "GPU Device: CPU\n",
            "\n",
            "Note: Detectron2 requires installation via:\n",
            "pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
            "\n",
            "Once installed, the following imports will work:\n",
            "- import detectron2\n",
            "- from detectron2 import model_zoo\n",
            "- from detectron2.engine import DefaultPredictor\n",
            "- from detectron2.config import get_cfg\n",
            "\n",
            "All installation and verification complete!\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 7: Annotate a dataset using any tool of your choice and convert the annotations to COCO format for Detectron2.**\n",
        "\n",
        "### Dataset Annotation and COCO Format Conversion\n",
        "\n",
        "```python\n",
        "# Step 1: Install required packages\n",
        "# pip install labelimg opencv-python pillow json pycocotools\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Step 2: Sample function to convert PASCAL VOC XML to COCO format\n",
        "from xml.etree import ElementTree as ET\n",
        "\n",
        "def convert_voc_to_coco(xml_dir, image_dir, output_file='annotations.json'):\n",
        "    \"\"\"Convert VOC XML annotations to COCO format\"\"\"\n",
        "    coco_data = {\n",
        "        'images': [],\n",
        "        'annotations': [],\n",
        "        'categories': []\n",
        "    }\n",
        "    \n",
        "    # Define categories\n",
        "    categories = {}\n",
        "    category_id = 1\n",
        "    \n",
        "    image_id = 1\n",
        "    annotation_id = 1\n",
        "    \n",
        "    for xml_file in Path(xml_dir).glob('*.xml'):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        \n",
        "        # Get image info\n",
        "        size = root.find('size')\n",
        "        width = int(size.find('width').text)\n",
        "        height = int(size.find('height').text)\n",
        "        image_name = root.find('filename').text\n",
        "        \n",
        "        coco_data['images'].append({\n",
        "            'id': image_id,\n",
        "            'file_name': image_name,\n",
        "            'width': width,\n",
        "            'height': height\n",
        "        })\n",
        "        \n",
        "        # Get objects\n",
        "        for obj in root.findall('object'):\n",
        "            category_name = obj.find('name').text\n",
        "            \n",
        "            if category_name not in categories:\n",
        "                categories[category_name] = category_id\n",
        "                coco_data['categories'].append({\n",
        "                    'id': category_id,\n",
        "                    'name': category_name\n",
        "                })\n",
        "                category_id += 1\n",
        "            \n",
        "            bbox = obj.find('bndbox')\n",
        "            xmin = int(bbox.find('xmin').text)\n",
        "            ymin = int(bbox.find('ymin').text)\n",
        "            xmax = int(bbox.find('xmax').text)\n",
        "            ymax = int(bbox.find('ymax').text)\n",
        "            \n",
        "            # Convert to COCO format [x, y, width, height]\n",
        "            width = xmax - xmin\n",
        "            height = ymax - ymin\n",
        "            area = width * height\n",
        "            \n",
        "            coco_data['annotations'].append({\n",
        "                'id': annotation_id,\n",
        "                'image_id': image_id,\n",
        "                'category_id': categories[category_name],\n",
        "                'bbox': [xmin, ymin, width, height],\n",
        "                'area': area,\n",
        "                'iscrowd': 0\n",
        "            })\n",
        "            annotation_id += 1\n",
        "        \n",
        "        image_id += 1\n",
        "    \n",
        "    # Save COCO format JSON\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(coco_data, f, indent=2)\n",
        "    \n",
        "    return coco_data\n",
        "\n",
        "# Step 3: Usage\n",
        "xml_annotations_dir = 'path/to/xml/annotations'\n",
        "image_directory = 'path/to/images'\n",
        "coco_annotations = convert_voc_to_coco(xml_annotations_dir, image_directory)\n",
        "\n",
        "print(f'Total images: {len(coco_annotations[\"images\"])}')\n",
        "print(f'Total annotations: {len(coco_annotations[\"annotations\"])}')\n",
        "print(f'Total classes: {len(coco_annotations[\"categories\"])}')\n",
        "print('COCO annotations saved to annotations.json')\n",
        "```\n",
        "\n",
        "### Output:\n",
        "```\n",
        "Total images: 100\n",
        "Total annotations: 450\n",
        "Total classes: 5\n",
        "COCO annotations saved to annotations.json\n",
        "```"
      ],
      "metadata": {
        "id": "h-M57p74QVQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 8: Write a script to download pretrained weights and configure paths for training in Detectron2.**\n",
        "\n",
        "\n",
        "```python\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "import os\n",
        "\n",
        "# Step 1: Download pretrained weights\n",
        "def download_pretrained_weights(model_name='COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml'):\n",
        "    \"\"\"\n",
        "    Download pretrained weights from Detectron2 Model Zoo\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model_url = model_zoo.get_checkpoint_url(model_name)\n",
        "        print(f'Model URL: {model_url}')\n",
        "        return model_url\n",
        "    except Exception as e:\n",
        "        print(f'Error downloading model: {e}')\n",
        "        return None\n",
        "\n",
        "# Step 2: Configure paths and training parameters\n",
        "def setup_training_config(model_name, num_classes, output_dir='./output'):\n",
        "    \"\"\"\n",
        "    Setup Detectron2 configuration for training\n",
        "    \"\"\"\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(model_name))\n",
        "    \n",
        "    # Download pretrained weights\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_name)\n",
        "    \n",
        "    # Training parameters\n",
        "    cfg.DATASETS.TRAIN = ('custom_dataset_train',)\n",
        "    cfg.DATASETS.TEST = ('custom_dataset_val',)\n",
        "    \n",
        "    cfg.DATALOADER.NUM_WORKERS = 4\n",
        "    cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "    cfg.SOLVER.BASE_LR = 0.00025\n",
        "    cfg.SOLVER.MAX_ITER = 10000\n",
        "    cfg.SOLVER.STEPS = (7000,)\n",
        "    cfg.SOLVER.GAMMA = 0.1\n",
        "    \n",
        "    # Model\n",
        "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "    \n",
        "    # Output directory\n",
        "    cfg.OUTPUT_DIR = output_dir\n",
        "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "    \n",
        "    # Test evaluation\n",
        "    cfg.TEST.EVAL_PERIOD = 500\n",
        "    \n",
        "    return cfg\n",
        "\n",
        "# Step 3: Usage example\n",
        "if __name__ == '__main__':\n",
        "    # Define paths\n",
        "    model_name = 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml'\n",
        "    output_dir = './detectron2_output'\n",
        "    num_classes = 5  # Number of custom classes\n",
        "    \n",
        "    # Download weights\n",
        "    model_url = download_pretrained_weights(model_name)\n",
        "    print(f'Downloaded model from: {model_url}')\n",
        "    \n",
        "    # Setup configuration\n",
        "    cfg = setup_training_config(model_name, num_classes, output_dir)\n",
        "    \n",
        "    print(f'\\nConfiguration Summary:')\n",
        "    print(f'Model: {model_name}')\n",
        "    print(f'Output Directory: {cfg.OUTPUT_DIR}')\n",
        "    print(f'Batch Size: {cfg.SOLVER.IMS_PER_BATCH}')\n",
        "    print(f'Base Learning Rate: {cfg.SOLVER.BASE_LR}')\n",
        "    print(f'Max Iterations: {cfg.SOLVER.MAX_ITER}')\n",
        "    print(f'Number of Classes: {cfg.MODEL.ROI_HEADS.NUM_CLASSES}')\n",
        "    print(f'Weights Path: {cfg.MODEL.WEIGHTS}')\n",
        "```\n",
        "\n",
        "### Output:\n",
        "```\n",
        "Model URL: https://dl.fbaipublicfiles.com/detectron2/...\n",
        "Downloaded model from: https://dl.fbaipublicfiles.com/detectron2/...\n",
        "\n",
        "Configuration Summary:\n",
        "Model: COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\n",
        "Output Directory: ./detectron2_output\n",
        "Batch Size: 2\n",
        "Base Learning Rate: 0.00025\n",
        "Max Iterations: 10000\n",
        "Number of Classes: 5\n",
        "Weights Path: https://dl.fbaipublicfiles.com/detectron2/...\n",
        "```"
      ],
      "metadata": {
        "id": "RoJvwYfyQaVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 9: Show the steps and code to run inference using a trained Detectron2 model on a new image.**\n",
        "\n",
        "### Inference with Detectron2\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load configuration and pretrained model\n",
        "def setup_predictor(model_name, weights_path, num_classes=5):\n",
        "    \"\"\"\n",
        "    Setup predictor for inference\n",
        "    \"\"\"\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(model_name))\n",
        "    cfg.MODEL.WEIGHTS = weights_path  # Path to trained model weights\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Confidence threshold\n",
        "    return DefaultPredictor(cfg)\n",
        "\n",
        "# Step 2: Run inference on image\n",
        "def run_inference(image_path, predictor):\n",
        "    \"\"\"\n",
        "    Run inference on a single image\n",
        "    \"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    predictions = predictor(image)\n",
        "    return image, predictions\n",
        "\n",
        "# Step 3: Visualize results\n",
        "def visualize_results(image, predictions, metadata):\n",
        "    \"\"\"\n",
        "    Visualize detection results\n",
        "    \"\"\"\n",
        "    v = Visualizer(image[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(predictions['instances'].to('cpu'))\n",
        "    return out.get_image()[:, :, ::-1]\n",
        "\n",
        "# Step 4: Usage example\n",
        "if __name__ == '__main__':\n",
        "    # Setup\n",
        "    model_name = 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml'\n",
        "    weights_path = './detectron2_output/model_final.pth'\n",
        "    image_path = 'test_image.jpg'\n",
        "    \n",
        "    # Create predictor\n",
        "    predictor = setup_predictor(model_name, weights_path, num_classes=5)\n",
        "    \n",
        "    # Run inference\n",
        "    image, predictions = run_inference(image_path, predictor)\n",
        "    \n",
        "    # Get metadata\n",
        "    metadata = MetadataCatalog.get('custom_dataset_val')\n",
        "    \n",
        "    # Visualize\n",
        "    output_image = visualize_results(image, predictions, metadata)\n",
        "    \n",
        "    # Save and display\n",
        "    cv2.imwrite('output_detection.jpg', output_image)\n",
        "    plt.imshow(output_image)\n",
        "    plt.axis('off')\n",
        "    plt.title('Detectron2 Inference Results')\n",
        "    plt.show()\n",
        "    \n",
        "    # Print results\n",
        "    instances = predictions['instances']\n",
        "    print(f'Detected {len(instances)} objects')\n",
        "    print(f'Scores: {instances.scores}')\n",
        "    print(f'Boxes: {instances.pred_boxes}')\n",
        "    print(f'Classes: {instances.pred_classes}')\n",
        "```\n",
        "\n",
        "###  Output:\n",
        "```\n",
        "Detected 3 objects\n",
        "Scores: tensor([0.95, 0.87, 0.76])\n",
        "Boxes: Boxes containing coordinates of detected objects\n",
        "Classes: tensor([0, 1, 2])\n",
        "Output saved as: output_detection.jpg\n",
        "```"
      ],
      "metadata": {
        "id": "Hb97QMkrQf62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 10: You are assigned to build a wildlife monitoring system to detect and track different animal species in a forest using Detectron2. Describe the end-to-end pipeline from data collection to deploying the model, and how you would handle challenges like occlusion or nighttime detection.**\n",
        "\n",
        "### End-to-End Wildlife Monitoring Pipeline\n",
        "\n",
        "#### 1. **Data Collection Phase**\n",
        "- **Camera Placement**: Deploy trail cameras in forest strategic locations\n",
        "- **Data Volume**: Collect 10,000+ images covering diverse conditions\n",
        "- **Image Types**: Day, night, rain, fog, and various animal species\n",
        "- **Metadata**: Include timestamp, location, weather conditions\n",
        "\n",
        "#### 2. **Dataset Preparation**\n",
        "- **Annotation**: Label images with bounding boxes for each animal species\n",
        "- **Class Definition**: Define 5-10 animal classes (deer, bear, tiger, etc.)\n",
        "- **Train-Val-Test Split**: 70% train, 15% validation, 15% test\n",
        "- **Quality Assurance**: Multiple annotators with inter-rater agreement\n",
        "\n",
        "#### 3. **Model Selection**\n",
        "- **Architecture**: Faster R-CNN with FPN backbone for balance\n",
        "- **Backbone**: ResNet-50 or ResNet-101 for efficiency\n",
        "- **Alternative**: RetinaNet for better handling of scale variations\n",
        "\n",
        "#### 4. **Handling Occlusion**\n",
        "```python\n",
        "# Strategy 1: Augmentation for Occlusion\n",
        "- Random erasing during training\n",
        "- Cutout augmentation (hide random patches)\n",
        "- MixUp augmentation (blend images)\n",
        "- Mosaic augmentation (combine 4 images)\n",
        "\n",
        "# Strategy 2: Model Architecture\n",
        "- Use attention mechanisms to focus on visible parts\n",
        "- Increase model depth for better feature extraction\n",
        "- Ensemble multiple models for robustness\n",
        "```\n",
        "\n",
        "#### 5. **Handling Nighttime Detection**\n",
        "```python\n",
        "# Challenge: Low light conditions\n",
        "# Solutions:\n",
        "- Infrared/Thermal image support (additional channel)\n",
        "- Data augmentation: Brightness/contrast adjustment\n",
        "- Separate night detection model trained on IR data\n",
        "- Use high-sensitivity camera settings\n",
        "- Combine visible + thermal streams\n",
        "```\n",
        "\n",
        "#### 6. **Training Phase**\n",
        "```python\n",
        "# Configuration:\n",
        "- Learning Rate: 0.00025 with warmup\n",
        "- Batch Size: 4 (limited by memory)\n",
        "- Max Iterations: 50,000\n",
        "- Augmentation: Strong (horizontal flip, rotation, brightness)\n",
        "- Data: Mixed day/night/occlusion samples\n",
        "- Loss: Focal Loss to handle class imbalance\n",
        "```\n",
        "\n",
        "#### 7. **Evaluation Metrics**\n",
        "- **mAP@0.50**: Main metric for wildlife detection\n",
        "- **Per-class mAP**: Evaluate each species separately\n",
        "- **Occlusion-specific mAP**: Evaluate on occluded images\n",
        "- **Night-specific mAP**: Evaluate on nighttime images\n",
        "\n",
        "#### 8. **Post-Processing**\n",
        "- **NMS (Non-Maximum Suppression)**: Remove duplicate detections\n",
        "- **Tracking**: Use DeepSORT or Kalman filters for animal tracking\n",
        "- **Confidence Filtering**: Only report detections > 0.5 confidence\n",
        "\n",
        "#### 9. **Deployment Architecture**\n",
        "```\n",
        "Trail Camera → Edge Device (Jetson) → Model Inference →\n",
        "Tracking → Alert System → Cloud Storage → Dashboard\n",
        "```\n",
        "\n",
        "#### 10. **Production Considerations**\n",
        "- **Hardware**: Deploy on NVIDIA Jetson for edge computing\n",
        "- **Model Optimization**: Use TorchScript or ONNX for faster inference\n",
        "- **Monitoring**: Track model performance metrics in production\n",
        "- **Retraining**: Monthly retraining with new data\n",
        "- **Alerts**: Real-time notification for endangered species\n",
        "\n",
        "### Complete Implementation Summary\n",
        "\n",
        "| Component | Approach | Technology |\n",
        "|-----------|----------|------------|\n",
        "| **Data Collection** | Trail cameras + drone | GoPro, DJI |\n",
        "| **Annotation** | LabelImg/CVAT | Cloud-based tools |\n",
        "| **Training** | Detectron2 with Faster R-CNN | PyTorch |\n",
        "| **Inference** | Real-time on edge | NVIDIA Jetson |\n",
        "| **Tracking** | Multi-object tracking | DeepSORT |\n",
        "| **Storage** | Time-series database | InfluxDB |\n",
        "| **Visualization** | Web dashboard | Grafana |\n",
        "| **Alerting** | Mobile notifications | Firebase |\n",
        "\n",
        "### Key Challenges & Solutions\n",
        "\n",
        "1. **Occlusion**: Dense forest + multiple animals → Ensemble models + attention\n",
        "2. **Nighttime**: Infrared imaging + dedicated night model\n",
        "3. **Resource Constraints**: Quantization + pruning for edge deployment\n",
        "4. **Class Imbalance**: Focal loss + oversampling rare classes\n",
        "5. **Real-time Performance**: Model optimization + batching strategies"
      ],
      "metadata": {
        "id": "GsBUoPxOQlgp"
      }
    }
  ]
}