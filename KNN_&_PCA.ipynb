{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **KNN & PCA | Vikash Kumar | wiryvikash15@gmail.com**"
      ],
      "metadata": {
        "id": "_me1h8ligz3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?**\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a simple yet powerful supervised machine learning algorithm that is non-parametric and an instance of lazy learning. It's called \"lazy\" because it doesn't build a model during the training phase; instead, it stores the entire training dataset. The actual work happens during prediction.\n",
        "\n",
        "The core idea is to predict the label of a new data point based on the labels of its \"k\" closest neighbors in the feature space.\n",
        "\n",
        "For Classification:\n",
        "To classify a new data point, the KNN algorithm identifies the 'k' nearest training data points. The new point is then assigned to the class that represents the majority among those 'k' neighbors. For example, if k=5 and three of the five nearest neighbors belong to Class A and two belong to Class B, the new point is classified as Class A.\n",
        "\n",
        "For Regression:\n",
        "To predict a continuous value for a new data point, the algorithm again finds the 'k' nearest neighbors. The final prediction is the average (or mean) of the values of these 'k' neighbors. For instance, if k=5 and the house prices of the five nearest neighbors are $250k, $260k, $270k, $280k, and $300k, the predicted price for the new house would be the average of these values."
      ],
      "metadata": {
        "id": "QSMdRhX4aTj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What is the Curse of Dimensionality and how does it affect KNN performance?**\n",
        "\n",
        "The Curse of Dimensionality refers to various problems that arise when analyzing data in high-dimensional spaces (i.e., datasets with a very large number of features). As the number of dimensions increases, the volume of the space increases so exponentially that the available data becomes sparse.\n",
        "\n",
        "This phenomenon severely affects the performance of KNN in several ways:\n",
        "\n",
        "- Distance Metrics Become Meaningless: In high dimensions, the distance between any two points in the dataset tends to become almost equal. When all points are equidistant from each other, the concept of a \"nearest neighbor\" becomes ambiguous and unreliable, undermining the core logic of KNN.\n",
        "\n",
        "- Data Sparsity: With more dimensions, we need exponentially more data to maintain the same density of data points. For a fixed amount of data, the points become very far apart, making it difficult to find a meaningful local neighborhood.\n",
        "\n",
        "- Increased Computational Cost: KNN's prediction phase requires calculating the distance from the new point to every single point in the training data. As the number of dimensions (features) increases, this distance calculation becomes much more computationally expensive.\n",
        "\n",
        "- Overfitting: With a large number of features, there's a higher chance that the algorithm will rely on irrelevant features to determine similarity, leading to poor generalization on unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "-GEwFoVdab2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What is Principal Component Analysis (PCA)? How is it different from feature selection?**\n",
        "\n",
        "Principal Component Analysis (PCA) is a popular dimensionality reduction technique. Its goal is to transform a large set of correlated variables into a smaller set of new, uncorrelated variables called principal components. These principal components are linear combinations of the original features and are ordered so that the first few components retain most of the variance (i.e., information) present in the original dataset.\n",
        "\n",
        "The key difference between PCA and feature selection lies in how they reduce dimensionality:\n",
        "\n",
        "**Principal Component Analysis (PCA)**\n",
        "\n",
        "- Feature Transformation: It creates new features (principal components) by combining the original ones.\n",
        "\n",
        "- The new features are linear combinations of all original features. They are often less interpretable.\n",
        "\n",
        "- It tries to preserve as much information (variance) as possible from the original dataset in fewer dimensions.\n",
        "\n",
        "- To reduce dimensionality while retaining the maximum possible variance.\n",
        "\n",
        "**Feature Selection**\n",
        "\n",
        "- Feature Subset: It selects a subset of the original features and discards the rest.\n",
        "\n",
        "- The output features are the original features, so they retain their original meaning and interpretability.\n",
        "\n",
        "- Information from the discarded features is completely lost.\n",
        "\n",
        "- To reduce dimensionality by finding the most relevant features and removing redundant or irrelevant ones.\n",
        "\n",
        "\n",
        "\n",
        "In short, PCA transforms the data into a new, smaller feature space, while feature selection keeps or discards original features.\n"
      ],
      "metadata": {
        "id": "6COwnpkTanOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What are eigenvalues and eigenvectors in PCA, and why are they important?**\n",
        "\n",
        "In the context of PCA, eigenvectors and eigenvalues are derived from the covariance matrix of the data. They are fundamental to identifying the principal components.\n",
        "\n",
        "- Eigenvectors: An eigenvector represents a direction in the feature space. For PCA, the eigenvectors of the covariance matrix point in the directions of maximum variance in the data. The first principal component is the direction defined by the first eigenvector.\n",
        "\n",
        "- Eigenvalues: An eigenvalue is a scalar value that indicates the magnitude or amount of variance in the data along its corresponding eigenvector.\n",
        "\n",
        "**Importance in PCA:**\n",
        "\n",
        "Eigenvalues and eigenvectors are crucial because they allow us to rank the principal components. The eigenvector with the highest eigenvalue is the direction that captures the most variance in the dataset, and it becomes the first principal component (PC1). The eigenvector with the second-highest eigenvalue becomes the second principal component (PC2), and so on.\n",
        "\n",
        "By sorting the eigenvalues in descending order, we can decide how many principal components to keep. We can choose to keep the top 'n' components that collectively explain a significant portion of the total variance, effectively reducing dimensionality while losing minimal information."
      ],
      "metadata": {
        "id": "Dvrjco6DcY6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. How do KNN and PCA complement each other when applied in a single pipeline?**\n",
        "\n",
        "KNN and PCA complement each other perfectly because PCA directly addresses KNN's primary weakness: the Curse of Dimensionality.\n",
        "\n",
        "Here's how they work together in a pipeline:\n",
        "\n",
        "- PCA Tackles High Dimensionality: KNN's performance degrades in high-dimensional spaces. PCA is applied first to reduce the number of features by transforming the data into a smaller set of principal components that capture most of the original variance.\n",
        "\n",
        "- More Meaningful Distances: By reducing the dimensions, PCA makes the data less sparse. This ensures that distance metrics (like Euclidean distance) used by KNN are more meaningful and reliable, leading to better classification or regression performance.\n",
        "\n",
        "- Reduced Computational Cost: KNN can be slow on datasets with many features because it needs to compute distances across all dimensions. By running KNN on the reduced dataset from PCA, the prediction time is significantly reduced.\n",
        "\n",
        "- Noise Reduction: PCA can act as a de-noising filter. The principal components with very small eigenvalues often represent noise in the data. By discarding them, PCA provides a cleaner dataset for the KNN algorithm to work with.\n",
        "\n",
        "The typical pipeline is:\n",
        "Scale Data → Apply PCA for dimensionality reduction → Train KNN on the transformed, lower-dimensional data.\n",
        "\n",
        "This combination leverages the strengths of PCA (dimensionality reduction) to mitigate the weaknesses of KNN (sensitivity to high dimensions), resulting in a faster and often more accurate model.\n",
        "\n"
      ],
      "metadata": {
        "id": "584h6Xg_cpGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.**"
      ],
      "metadata": {
        "id": "BmS7sy1xdFWe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhxXoHgmaPZi",
        "outputId": "9d8d81df-cf67-4eda-c951-7e53f2cd113b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training KNN without Feature Scaling ---\n",
            "Accuracy without scaling: 0.7407\n",
            "\n",
            "--- Training KNN with Feature Scaling ---\n",
            "Accuracy with scaling: 0.9630\n",
            "\n",
            "--- Comparison ---\n",
            "The model accuracy improved from 0.7407 to 0.9630 after feature scaling.\n",
            "This demonstrates that scaling is crucial for distance-based algorithms like KNN.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"--- Training KNN without Feature Scaling ---\")\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(f\"Accuracy without scaling: {accuracy_unscaled:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training KNN with Feature Scaling ---\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")\n",
        "\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(f\"The model accuracy improved from {accuracy_unscaled:.4f} to {accuracy_scaled:.4f} after feature scaling.\")\n",
        "print(\"This demonstrates that scaling is crucial for distance-based algorithms like KNN.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.**"
      ],
      "metadata": {
        "id": "LpoQpJAxdNeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "print(\"Explained Variance Ratio of Each Principal Component:\")\n",
        "for i, ratio in enumerate(explained_variance_ratio):\n",
        "    print(f\"  PC-{i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nCumulative Explained Variance:\")\n",
        "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
        "for i, cum_var in enumerate(cumulative_variance):\n",
        "    print(f\"  Up to PC-{i+1}: {cum_var:.4f} ({cum_var*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nThe first two components alone explain {cumulative_variance[1]*100:.2f}% of the total variance.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmcGDrFJdJNN",
        "outputId": "ec060780-71bd-4dc5-eb53-b7098a7e5ef8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of Each Principal Component:\n",
            "  PC-1: 0.3620 (36.20%)\n",
            "  PC-2: 0.1921 (19.21%)\n",
            "  PC-3: 0.1112 (11.12%)\n",
            "  PC-4: 0.0707 (7.07%)\n",
            "  PC-5: 0.0656 (6.56%)\n",
            "  PC-6: 0.0494 (4.94%)\n",
            "  PC-7: 0.0424 (4.24%)\n",
            "  PC-8: 0.0268 (2.68%)\n",
            "  PC-9: 0.0222 (2.22%)\n",
            "  PC-10: 0.0193 (1.93%)\n",
            "  PC-11: 0.0174 (1.74%)\n",
            "  PC-12: 0.0130 (1.30%)\n",
            "  PC-13: 0.0080 (0.80%)\n",
            "\n",
            "Cumulative Explained Variance:\n",
            "  Up to PC-1: 0.3620 (36.20%)\n",
            "  Up to PC-2: 0.5541 (55.41%)\n",
            "  Up to PC-3: 0.6653 (66.53%)\n",
            "  Up to PC-4: 0.7360 (73.60%)\n",
            "  Up to PC-5: 0.8016 (80.16%)\n",
            "  Up to PC-6: 0.8510 (85.10%)\n",
            "  Up to PC-7: 0.8934 (89.34%)\n",
            "  Up to PC-8: 0.9202 (92.02%)\n",
            "  Up to PC-9: 0.9424 (94.24%)\n",
            "  Up to PC-10: 0.9617 (96.17%)\n",
            "  Up to PC-11: 0.9791 (97.91%)\n",
            "  Up to PC-12: 0.9920 (99.20%)\n",
            "  Up to PC-13: 1.0000 (100.00%)\n",
            "\n",
            "The first two components alone explain 55.41% of the total variance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.**"
      ],
      "metadata": {
        "id": "x17Pv0iLdU6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "accuracy_original_scaled = accuracy_score(y_test, knn_original.predict(X_test_scaled))\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(\"--- Accuracy Comparison ---\")\n",
        "print(f\"Original number of features: {X.shape[1]}\")\n",
        "print(f\"Number of features after PCA: {X_train_pca.shape[1]}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Accuracy on original (scaled) dataset: {accuracy_original_scaled:.4f}\")\n",
        "print(f\"Accuracy on PCA-transformed dataset:   {accuracy_pca:.4f}\")\n",
        "print(\"\\nConclusion: The accuracy is very similar, but the PCA model is much faster and simpler,\")\n",
        "print(\"as it uses only 2 features instead of 13.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCRQucVTdP9O",
        "outputId": "be776b2a-52f8-4c53-81e1-ce36d3daa06f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Accuracy Comparison ---\n",
            "Original number of features: 13\n",
            "Number of features after PCA: 2\n",
            "------------------------------\n",
            "Accuracy on original (scaled) dataset: 0.9630\n",
            "Accuracy on PCA-transformed dataset:   0.9815\n",
            "\n",
            "Conclusion: The accuracy is very similar, but the PCA model is much faster and simpler,\n",
            "as it uses only 2 features instead of 13.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.**"
      ],
      "metadata": {
        "id": "W2aVE_rSdchL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(\"--- KNN Accuracy with Different Distance Metrics ---\")\n",
        "print(f\"Accuracy with Euclidean distance: {accuracy_euclidean:.4f}\")\n",
        "print(f\"Accuracy with Manhattan distance: {accuracy_manhattan:.4f}\")\n",
        "\n",
        "if accuracy_euclidean > accuracy_manhattan:\n",
        "    print(\"\\nEuclidean distance performed slightly better in this case.\")\n",
        "elif accuracy_manhattan > accuracy_euclidean:\n",
        "    print(\"\\nManhattan distance performed slightly better in this case.\")\n",
        "else:\n",
        "    print(\"\\nBoth distance metrics achieved the same accuracy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwKlBoHfdYIN",
        "outputId": "35decaba-2d05-4aec-b686-d88ff45957e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- KNN Accuracy with Different Distance Metrics ---\n",
            "Accuracy with Euclidean distance: 0.9630\n",
            "Accuracy with Manhattan distance: 0.9630\n",
            "\n",
            "Both distance metrics achieved the same accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.**\n",
        "\n",
        "**Due to the large number of features and a small number of samples, traditional models overfit.**\n",
        "\n",
        "**Explain how you would:**\n",
        "\n",
        "**- Use PCA to reduce dimensionality**\n",
        "\n",
        "**- Decide how many components to keep**\n",
        "\n",
        "**- Use KNN for classification post-dimensionality reduction**\n",
        "\n",
        "**- Evaluate the model**\n",
        "\n",
        "**- Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data**"
      ],
      "metadata": {
        "id": "h4T_g6aPdoII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a high-dimensional gene expression dataset with a small number of samples, a combined PCA and KNN pipeline is an excellent strategy to build a robust classifier while avoiding overfitting. Here is the step-by-step approach.\n",
        "\n",
        "**1. Use PCA to Reduce Dimensionality**\n",
        "\n",
        "The first and most critical step is to tackle the \"curse of dimensionality.\"\n",
        "\n",
        "- Data Scaling: Before applying PCA, it is absolutely essential to scale the data. I would use StandardScaler to ensure each gene expression feature has a mean of 0 and a standard deviation of 1. This prevents features with larger variances from dominating the PCA process.\n",
        "\n",
        "- Applying PCA: I would then fit PCA on the scaled training data. This will transform the thousands of correlated gene features into a much smaller set of uncorrelated principal components, which represent the most significant patterns of variation in the gene expression data.\n",
        "\n",
        "**2. Decide How Many Components to Keep**\n",
        "\n",
        "Choosing the right number of components is a trade-off between information retention and dimensionality reduction. I would use two methods to make an informed decision:\n",
        "\n",
        "- Explained Variance Threshold: I would calculate the cumulative explained variance and choose the minimum number of components required to capture a high percentage of the total variance, typically 95%. This ensures we retain most of the useful signal while discarding noise.\n",
        "\n",
        "- Scree Plot: I would visualize the eigenvalues of each component in a \"scree plot.\" This plot typically shows a steep curve followed by an \"elbow\" where the curve flattens out. The number of components at this elbow point is often a good choice, as it represents the point of diminishing returns where subsequent components explain very little new variance.\n",
        "\n",
        "**3. Use KNN for Classification Post-Dimensionality Reduction**\n",
        "\n",
        "Once the dimensionality has been reduced, I would train a K-Nearest Neighbors (KNN) classifier.\n",
        "\n",
        "- Training: The KNN model would be trained on the transformed dataset (the selected principal components). This model will be computationally much faster and less prone to overfitting because it's operating in a lower-dimensional space where distance metrics are more meaningful.\n",
        "\n",
        "- Hyperparameter Tuning: I would use GridSearchCV with cross-validation to find the optimal value of 'k' (number of neighbors) and the best distance metric (euclidean vs. manhattan).\n",
        "\n",
        "**4. Evaluate the Model**\n",
        "\n",
        "Given the small sample size, a simple train-test split is insufficient.\n",
        "\n",
        "- Evaluation Protocol: I would use Stratified k-Fold Cross-Validation to get a reliable estimate of the model's performance. Stratification ensures that the proportion of each cancer type is the same in each fold, which is critical for imbalanced datasets.\n",
        "\n",
        "- Evaluation Metrics: I would not rely solely on accuracy. A comprehensive evaluation would include:\n",
        "\n",
        "    - Confusion Matrix: To see the exact breakdown of correct and incorrect predictions for each cancer type.\n",
        "\n",
        "    - Precision, Recall, and F1-Score (per class): To assess performance for each specific type of cancer, which is vital in a medical context.\n",
        "\n",
        "\n",
        "**5. Justify this Pipeline to Stakeholders**\n",
        "\n",
        "\n",
        "When presenting this solution, I would emphasize the following points:\n",
        "\n",
        "- Robustness: \"This pipeline directly addresses the main challenge of this dataset: having far more features than samples. By using PCA, we create a model that learns from the most important underlying biological patterns, not the noise, making it less likely to overfit and more reliable for diagnosing new patients.\"\n",
        "\n",
        "- Efficiency: \"By reducing thousands of gene features to a handful of principal components, the final classification model is extremely fast and computationally inexpensive, making it practical for real-world clinical use.\"\n",
        "\n",
        "- Proven Methodology: \"This combination of PCA and KNN is a standard and well-regarded approach in bioinformatics for analyzing complex genetic data. It allows us to build a powerful predictive model even when the data is challenging, leading to a more accurate and trustworthy diagnostic tool.\""
      ],
      "metadata": {
        "id": "lCl8DL9Id_UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lbhv2JU2dfF5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}