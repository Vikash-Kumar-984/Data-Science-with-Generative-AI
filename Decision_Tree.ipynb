{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Decision Tree | Vikash Kumar | wiryvikash15@gmail.com**"
      ],
      "metadata": {
        "id": "Vdip87fn5Tz1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91f4raottjsl"
      },
      "source": [
        "**1. What is a Decision Tree, and how does it work in the context of classification?**\n",
        "\n",
        "A **Decision Tree** is a supervised machine learning algorithm with a flowchart-like structure. Each **internal node** represents a test on a feature, each **branch** represents the outcome of the test, and each **leaf node** represents a final class label or value. It's used for both classification and regression tasks.\n",
        "\n",
        "In classification, a decision tree works by recursively splitting the dataset into smaller, more homogeneous subsets based on the most significant features. The goal is to create leaf nodes that are as \"pure\" as possible, meaning they contain data points from a single class.\n",
        "\n",
        "The process is as follows:\n",
        "1.  **Find the Best Split**: The algorithm starts at the root and selects the feature and threshold that best separates the data into distinct classes. This is often done by maximizing **Information Gain** or minimizing an impurity measure like **Gini Impurity** or **Entropy**.\n",
        "2.  **Partition the Data**: The data is split into child nodes based on the chosen feature's test.\n",
        "3.  **Repeat Recursively**: This process is repeated for each child node until a stopping condition is met (e.g., the node is pure, the tree reaches a maximum depth, or a node has too few samples to split).\n",
        "4.  **Classify**: To classify a new data point, it traverses the tree from the root down, following the test conditions until it reaches a leaf node. The class label of that leaf node is the final prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hCLm2-Gtjsp"
      },
      "source": [
        "**2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "**Gini Impurity** and **Entropy** are metrics used to measure the level of impurity or disorder within a set of data points. In a decision tree, the goal is to choose splits that result in child nodes with lower impurity than the parent node.\n",
        "\n",
        "### Gini Impurity\n",
        "Gini Impurity measures the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the class distribution in the subset. The formula is:\n",
        "$$Gini = 1 - \\sum_{i=1}^{C} (p_i)^2$$\n",
        "Where $p_i$ is the probability of an element belonging to class $i$.\n",
        "* A Gini score of **0** represents a pure node (all elements belong to one class).\n",
        "* A Gini score of **0.5** (for a binary classification) represents maximum impurity (elements are equally distributed among classes).\n",
        "\n",
        "### Entropy\n",
        "Entropy is a concept from information theory that measures the amount of uncertainty or randomness in a set of data. The formula is:\n",
        "$$Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)$$\n",
        "Where $p_i$ is the probability of an element belonging to class $i$.\n",
        "* An Entropy of **0** represents a pure node.\n",
        "* An Entropy of **1** (for a binary classification) represents maximum impurity.\n",
        "\n",
        "### Impact on Splits\n",
        "Both metrics guide the decision tree's construction by evaluating the quality of a potential split. The algorithm calculates the impurity of the parent node and the weighted average impurity of the potential child nodes for every possible split. It then chooses the split that results in the largest reduction in impurity. This reduction is known as **Information Gain** (when using Entropy). A larger reduction signifies a better, more informative split. While computationally slightly different, Gini Impurity and Entropy generally produce very similar trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8gAs1Omtjsp"
      },
      "source": [
        "**3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n",
        "\n",
        "**Pre-pruning** and **post-pruning** are two techniques used to prevent a decision tree from overfitting, which occurs when the model learns the training data too well, including its noise, and fails to generalize to new data.\n",
        "\n",
        "### Pre-Pruning (Early Stopping)\n",
        "Pre-pruning involves stopping the tree's growth *before* it becomes fully grown and complex. This is done by setting stopping conditions, such as:\n",
        "* `max_depth`: Limiting the maximum depth of the tree.\n",
        "* `min_samples_split`: Setting the minimum number of samples required to split an internal node.\n",
        "* `min_samples_leaf`: Setting the minimum number of samples required to be at a leaf node.\n",
        "\n",
        " **Practical Advantage**: Pre-pruning is computationally efficient because it avoids generating overly complex parts of the tree that would be removed later anyway.\n",
        "\n",
        "### Post-Pruning (Pruning)\n",
        "Post-pruning involves growing the tree to its full complexity first and then removing (pruning) branches that provide little predictive power. The algorithm prunes nodes from the bottom up, replacing them with a leaf node if the change results in a better-performing, simpler model on a validation set. A common technique is **Cost Complexity Pruning**.\n",
        "\n",
        " **Practical Advantage**: Post-pruning can lead to a more optimal and accurate tree because it makes pruning decisions based on the performance of the fully grown tree, rather than stopping prematurely based on a heuristic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri7i-QOitjsr"
      },
      "source": [
        "**4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "\n",
        "**Information Gain** is the metric used to select the best feature to split on at each step of building a decision tree. It measures the reduction in **Entropy** (or impurity) achieved by partitioning a dataset based on a particular feature.\n",
        "\n",
        "The formula for Information Gain is:\n",
        "$$\\text{Information Gain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\text{Entropy}(S_v)$$\n",
        "Where:\n",
        "* $S$ is the original dataset.\n",
        "* $A$ is the feature being tested.\n",
        "* $\\text{Entropy}(S)$ is the entropy of the original dataset.\n",
        "* $\\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\text{Entropy}(S_v)$ is the weighted average entropy of the subsets created by the split.\n",
        "\n",
        "**Importance for Choosing the Best Split:**\n",
        "Information Gain is crucial because it provides a quantitative measure for how well a feature separates the training examples according to their target class. At each node, the algorithm calculates the Information Gain for every possible feature split and selects the feature that **maximizes** this gain. By doing so, it prioritizes splits that create the most homogeneous (purest) child nodes, leading to a more accurate and efficient tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7VsmpuItjsr"
      },
      "source": [
        "**5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**\n",
        "\n",
        "**Dataset Info:**\n",
        "\n",
        "- **Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV)**\n",
        "- **Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).**\n",
        "\n",
        "**Common Real-World Applications**\n",
        "\n",
        "1.  **Healthcare**: Diagnosing diseases by classifying patient symptoms and lab results.\n",
        "2.  **Finance**: Predicting loan defaults or credit risk based on an applicant's financial history.\n",
        "3.  **Marketing**: Identifying potential customers (customer segmentation) based on demographic and purchasing data.\n",
        "4.  **Manufacturing**: Detecting faulty products through quality control analysis.\n",
        "\n",
        "**Main Advantages**\n",
        "\n",
        "* **Easy to Understand**: The flowchart-like structure is intuitive and simple to visualize and interpret.\n",
        "* **Handles Mixed Data**: They can handle both numerical and categorical data without extensive preprocessing.\n",
        "* **Non-parametric**: They make no assumptions about the underlying distribution of the data.\n",
        "* **Feature Importance**: They inherently provide a measure of which features are most important for making predictions.\n",
        "\n",
        "**Main Limitations**\n",
        "\n",
        "* **Overfitting**: Decision trees can easily become too complex and memorize the training data, leading to poor performance on new data. Pruning is often required to mitigate this.\n",
        "* **Instability**: Small variations in the data can result in a completely different tree being generated.\n",
        "* **Bias towards Features with More Levels**: Features with many levels can be favored by impurity measures like Information Gain, which can lead to suboptimal splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cywfE4-stjss"
      },
      "source": [
        "**6. Write a Python program to:**\n",
        "\n",
        "- **load the Iris Dataset,**\n",
        "- **train a Decision Tree Classifier using the Gini criterion,**\n",
        "- **and print the model's accuracy and feature importances.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvLTBt9Ntjss",
        "outputId": "188050a2-c7e2-427d-f50f-eadb0a8a34dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 1.0000\n",
            "\n",
            "Feature Importances:\n",
            "             feature  importance\n",
            "2  petal length (cm)    0.893264\n",
            "3   petal width (cm)    0.087626\n",
            "1   sepal width (cm)    0.019110\n",
            "0  sepal length (cm)    0.000000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\\n\")\n",
        "\n",
        "importances = clf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "print(\"Feature Importances:\")\n",
        "print(feature_importance_df.sort_values(by='importance', ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY0YtD0etjst"
      },
      "source": [
        "**7. Write a Python program to:**\n",
        "\n",
        "- **load the Iris Dataset,**\n",
        "- **train a Decision Tree Classifier with `max_depth=3`, and compare its accuracy to a fully-grown tree.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H97l8hkctjsu",
        "outputId": "2d637c2f-c5ad-4330-f56b-c944edb9ac02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the fully-grown tree: 1.0000\n",
            "Accuracy of the tree with max_depth=3: 1.0000\n",
            "\n",
            "The pruned tree (max_depth=3) has the same accuracy as the fully-grown tree in this case.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy of the fully-grown tree: {accuracy_full:.4f}\")\n",
        "\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "y_pred_pruned = pruned_tree.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "print(f\"Accuracy of the tree with max_depth=3: {accuracy_pruned:.4f}\")\n",
        "\n",
        "print(f\"\\nThe pruned tree (max_depth=3) has the same accuracy as the fully-grown tree in this case.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88x3eJOVtjsu"
      },
      "source": [
        "**8. Write a Python program to:**\n",
        "\n",
        "- **load the Boston Housing Dataset,**\n",
        "\n",
        "- **train a Decision Tree Regressor,**\n",
        "- **and print the Mean Squared Error (MSE) and feature importances.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTTXRER9tjsu",
        "outputId": "69a65525-f1f1-4dcd-93b8-036d39c15294"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
            "C:\\Users\\wiryv\\AppData\\Local\\Temp\\ipykernel_17092\\2149051769.py:12: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error (MSE): 11.5880\n",
            "\n",
            "Feature Importances:\n",
            "    feature  importance\n",
            "5        RM    0.575807\n",
            "12    LSTAT    0.189980\n",
            "7       DIS    0.109624\n",
            "0      CRIM    0.058465\n",
            "10  PTRATIO    0.025043\n",
            "11        B    0.011873\n",
            "2     INDUS    0.009872\n",
            "6       AGE    0.007170\n",
            "4       NOX    0.007051\n",
            "9       TAX    0.002181\n",
            "8       RAD    0.001646\n",
            "1        ZN    0.000989\n",
            "3      CHAS    0.000297\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n",
        "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "X = pd.DataFrame(data, columns=feature_names)\n",
        "y = target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\\n\")\n",
        "\n",
        "importances = reg.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "print(\"Feature Importances:\")\n",
        "print(feature_importance_df.sort_values(by='importance', ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The Boston Housing dataset was removed from scikit-learn in version 1.2 due to ethical concerns."
      ],
      "metadata": {
        "id": "-xrJI-9b5ERl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0SydhMktjsv"
      },
      "source": [
        "**9. Write a Python program to:**\n",
        "\n",
        "- **load the Iris Dataset,**\n",
        "-  **tune the Decision Tree's `max_depth` and `min_samples_split` using GridSearchCV,**\n",
        "- **print the best parameters and the resulting model accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTIZ84o9tjsv",
        "outputId": "99fa61c0-2174-4ad2-b194-11fa5f21bb5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters found: {'max_depth': 3, 'min_samples_split': 2}\n",
            "\n",
            "Accuracy with best parameters: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(estimator=dtree, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best Parameters found: {grid_search.best_params_}\\n\")\n",
        "\n",
        "best_tree = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_tree.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with best parameters: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAehPux3tjsv"
      },
      "source": [
        "**10. Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.**\n",
        "\n",
        "**Explain the step-by-step process you would follow to:**\n",
        "\n",
        "- **Handle the missing values**\n",
        "- **Encode the categorical features**\n",
        "- **Train a Decision Tree model**\n",
        "- **Tune its hyperparameters**\n",
        "- **Evaluate its performance And describe what business value this model could provide in the real-world setting**\n",
        "\n",
        "The step-by-step process for building a decision tree model to predict whether a patient has a certain disease, addressing data preprocessing, training, evaluation, and business value.\n",
        "\n",
        "**Step 1: Data Preprocessing**\n",
        "\n",
        "1.  **Handle Missing Values**:\n",
        "    * **Numerical Data**: For features like age or blood pressure, I would use **mean or median imputation**. Median is often preferred if the data has outliers.\n",
        "    * **Categorical Data**: For features like blood type or gender, I would use **mode imputation** (filling with the most frequent value) or treat \"missing\" as a separate category if it's potentially informative.\n",
        "\n",
        "2.  **Encode Categorical Features**:\n",
        "    * Decision trees can handle categorical data, but `scikit-learn`'s implementation requires them to be numeric.\n",
        "    * For **nominal** features (no intrinsic order, e.g., 'Gender'), I would use **One-Hot Encoding**.\n",
        "    * For **ordinal** features (with a clear order, e.g., 'Pain Level' as Low, Medium, High), I would use **Label Encoding** (e.g., mapping to 0, 1, 2).\n",
        "\n",
        "**Step 2: Model Training**\n",
        "\n",
        "1.  **Split Data**: I would split the preprocessed dataset into a training set (typically 80%) and a testing set (20%) to evaluate the model's performance on unseen data.\n",
        "2.  **Train a Decision Tree Model**: I would initialize a `DecisionTreeClassifier` and fit it on the training data (`X_train`, `y_train`).\n",
        "\n",
        "**Step 3: Hyperparameter Tuning**\n",
        "\n",
        "To prevent overfitting and find the best model configuration, I would use **GridSearchCV** with **cross-validation**. This method exhaustively searches a specified parameter grid to find the optimal combination. Key hyperparameters to tune include:\n",
        "* `criterion`: 'gini' or 'entropy'.\n",
        "* `max_depth`: The maximum depth of the tree.\n",
        "* `min_samples_split`: The minimum number of samples needed to split a node.\n",
        "* `min_samples_leaf`: The minimum number of samples allowed in a leaf node.\n",
        "\n",
        "**Step 4: Performance Evaluation**\n",
        "\n",
        "After training and tuning, I would evaluate the final model on the unseen test set using several metrics:\n",
        "* **Accuracy**: Overall correct predictions.\n",
        "* **Precision**: Of the patients predicted to have the disease, how many actually do? (Minimizes false positives).\n",
        "* **Recall (Sensitivity)**: Of all the patients who actually have the disease, how many did the model correctly identify? (Minimizes false negatives).\n",
        "* **F1-Score**: The harmonic mean of Precision and Recall, providing a balanced measure.\n",
        "* **ROC Curve and AUC**: To visualize the trade-off between the true positive rate and false positive rate.\n",
        "\n",
        "**Step 5: Business Value**\n",
        "\n",
        "This predictive model would provide immense business value to the healthcare company:\n",
        "* **Early Diagnosis**: It could act as a preliminary screening tool, helping doctors identify at-risk patients earlier and more accurately.\n",
        "* **Resource Allocation**: By flagging high-risk patients, the hospital can prioritize resources like specialized tests, doctor consultations, and treatments more effectively.\n",
        "* **Improved Patient Outcomes**: Early intervention, guided by the model's predictions, can lead to better treatment success rates and improved patient health.\n",
        "* **Cost Reduction**: By catching the disease early, the model can help reduce the long-term costs associated with treating advanced stages of the illness."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}